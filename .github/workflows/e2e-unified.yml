name: E2E Unified Pipeline (Kernel + Userspace)

# Single workflow that runs BOTH userspace and kernel tests on the SAME machine
# for accurate, consistent performance comparison.
#
# Benefits:
# - Same hardware for both tests = fair comparison
# - Same system state = reduced variability
# - Single workflow = simpler, no data merging needed
# - Direct comparison in one report

on:
  schedule:
    - cron: '0 6 * * 0'  # Weekly on Sunday at 6 AM UTC
  workflow_dispatch:
    inputs:
      axon_version:
        description: 'Axon version to test'
        required: false
        default: 'v3.1.9'
      core_version:
        description: 'Core version to test'
        required: false
        default: '5.0.1-alpha'
      kernel_mode:
        description: 'Kernel module mode'
        required: true
        type: choice
        options:
          - basic       # Memory manager only
          - scheduler   # Memory + ML scheduler
          - full        # Memory + scheduler + GPU manager
        default: 'scheduler'
      models:
        description: 'Models to test (comma-separated, empty = all enabled)'
        required: false
        default: ''

permissions:
  contents: write
  pages: write
  id-token: write
  packages: read

env:
  AXON_VERSION: ${{ github.event.inputs.axon_version || 'v3.1.9' }}
  CORE_VERSION: ${{ github.event.inputs.core_version || '5.0.1-alpha' }}
  KERNEL_MODE: ${{ github.event.inputs.kernel_mode || 'scheduler' }}

jobs:
  unified-e2e-test:
    name: E2E Test (Unified - Kernel + Userspace)
    runs-on: [self-hosted, linux, kernel-capable]
    environment:
      name: github-pages

    steps:
      # ========================================================================
      # SETUP
      # ========================================================================
      - name: Checkout
        uses: actions/checkout@v4

      - name: Clean up from previous runs
        run: |
          echo "========================================"
          echo "Cleaning up stale state from previous runs"
          echo "========================================"

          # Stop any running Core processes
          pkill -f mlos_core 2>/dev/null || true
          pkill -f "mlos-core" 2>/dev/null || true
          sleep 2

          # Unload kernel module if loaded
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # Clean up caches
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true
          rm -rf ~/.axon/cache/models 2>/dev/null || true
          rm -rf ~/.axon/cache 2>/dev/null || true
          rm -rf ~/mlos-core/models 2>/dev/null || true

          # Clean up test directories
          rm -rf model-results-userspace model-results-kernel metrics output 2>/dev/null || true

          echo "Disk space available:"
          df -h /

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pyyaml jinja2 requests

      - name: Get enabled models from config
        id: get-models
        run: |
          if [ -n "${{ github.event.inputs.models }}" ]; then
            MODELS="${{ github.event.inputs.models }}"
          else
            MODELS=$(python3 -c "
          import yaml
          with open('config/models.yaml') as f:
              config = yaml.safe_load(f)
          # models.yaml uses dict structure: models: {model_name: {enabled: true, ...}}
          models_dict = config.get('models', {})
          models = [name for name, cfg in models_dict.items() if isinstance(cfg, dict) and cfg.get('enabled', False)]
          print(','.join(models))
          ")
          fi
          echo "models=$MODELS" >> $GITHUB_OUTPUT
          echo "Models to test: $MODELS"

      - name: Collect hardware info
        id: hardware
        run: |
          echo "os_name=$(lsb_release -ds 2>/dev/null || cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)" >> $GITHUB_OUTPUT
          echo "kernel_version=$(uname -r)" >> $GITHUB_OUTPUT
          echo "cpu_model=$(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> $GITHUB_OUTPUT
          echo "cpu_cores=$(nproc)" >> $GITHUB_OUTPUT
          echo "memory_gb=$(free -g | awk '/^Mem:/{print $2}')" >> $GITHUB_OUTPUT

          # GPU detection
          if command -v nvidia-smi &> /dev/null; then
            GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
            GPU_COUNT=$(nvidia-smi --query-gpu=count --format=csv,noheader | head -1)
          else
            GPU_NAME="None (CPU only)"
            GPU_COUNT="0"
          fi
          echo "gpu_name=$GPU_NAME" >> $GITHUB_OUTPUT
          echo "gpu_count=$GPU_COUNT" >> $GITHUB_OUTPUT

          echo "Hardware Info:"
          echo "  OS: $(lsb_release -ds 2>/dev/null || echo 'Linux')"
          echo "  CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
          echo "  Cores: $(nproc)"
          echo "  Memory: $(free -g | awk '/^Mem:/{print $2}') GB"
          echo "  GPU: $GPU_NAME"

      # ========================================================================
      # DOWNLOAD RELEASES
      # ========================================================================
      - name: Download Axon release
        run: |
          VERSION="${AXON_VERSION}"
          VERSION_NO_V=${VERSION#v}
          OS="linux"
          ARCH="amd64"

          # Axon uses underscore naming: axon_3.1.9_linux_amd64.tar.gz
          TARBALL="axon_${VERSION_NO_V}_${OS}_${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/${TARBALL}"

          echo "Downloading Axon from: $URL"
          curl -L -f -o "/tmp/${TARBALL}" "$URL"

          mkdir -p ~/.local/bin
          tar -xzf "/tmp/${TARBALL}" -C ~/.local/bin
          chmod +x ~/.local/bin/axon
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          ~/.local/bin/axon version

      - name: Download converter image
        run: |
          VERSION="${AXON_VERSION}"
          VERSION_NO_V=${VERSION#v}
          OS="linux"
          ARCH="amd64"

          CONVERTER="axon-converter-${VERSION_NO_V}-${OS}-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/${CONVERTER}"

          echo "Downloading converter image from: $URL"
          if curl -L -f -o "/tmp/${CONVERTER}" "$URL" 2>/dev/null; then
            echo "Loading Docker image..."
            docker load -i "/tmp/${CONVERTER}"

            # Tag for local use
            IMAGE_TAG="ghcr.io/mlos-foundation/axon-converter:${VERSION_NO_V}"
            docker tag "$IMAGE_TAG" "ghcr.io/mlos-foundation/axon-converter:latest" 2>/dev/null || true

            echo "Converter image loaded"
            docker images | grep converter || true
          else
            echo "Warning: Converter image not available for this version"
          fi

      - name: Download Core release
        run: |
          VERSION="${CORE_VERSION}"
          OS="linux"
          ARCH="amd64"

          # Core uses underscore naming: mlos-core_5.0.1-alpha_linux-amd64.tar.gz
          TARBALL="mlos-core_${VERSION}_${OS}-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/core/releases/download/v${VERSION}/${TARBALL}"

          echo "Downloading Core from: $URL"
          mkdir -p ~/mlos-core
          curl -L -f -o "/tmp/${TARBALL}" "$URL"
          tar -xzf "/tmp/${TARBALL}" -C ~/mlos-core

          # Set up ONNX runtime
          ONNX_DIR=$(find ~/mlos-core -type d -name "onnxruntime" | head -1)
          if [ -n "$ONNX_DIR" ]; then
            rm -rf ~/mlos-core/build/onnxruntime 2>/dev/null || true
            mkdir -p ~/mlos-core/build
            mv "$ONNX_DIR" ~/mlos-core/build/
            export LD_LIBRARY_PATH="$HOME/mlos-core/build/onnxruntime/lib:$LD_LIBRARY_PATH"
            echo "LD_LIBRARY_PATH=$HOME/mlos-core/build/onnxruntime/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          fi

          # Set up llama.cpp
          LLAMA_DIR=$(find ~/mlos-core -type d -name "llama.cpp" | head -1)
          if [ -n "$LLAMA_DIR" ]; then
            export LD_LIBRARY_PATH="$LLAMA_DIR/lib:$LD_LIBRARY_PATH"
            echo "LD_LIBRARY_PATH=$LLAMA_DIR/lib:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          fi

      # ========================================================================
      # PHASE 1: USERSPACE TESTS (No kernel module)
      # ========================================================================
      - name: "Phase 1: Start Core (Userspace Mode)"
        run: |
          echo "========================================"
          echo "PHASE 1: USERSPACE TESTS"
          echo "========================================"
          echo "Running without kernel module for baseline comparison"

          # Ensure kernel module is NOT loaded
          if lsmod | grep -q "mlos_ml\|mlos-ml"; then
            echo "Unloading kernel module for userspace baseline..."
            sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          fi

          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)

          echo "Starting Core in userspace mode..."
          nohup "$BINARY" > /tmp/core-userspace.log 2>&1 &
          echo $! > /tmp/core-userspace.pid

          # Wait for startup
          for i in {1..30}; do
            if curl -s http://localhost:8080/health > /dev/null 2>&1; then
              echo "Core started successfully (userspace mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 1: Run E2E tests (Userspace)"
        id: test-userspace
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-userspace
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (userspace): $MODELS"
          TEST_START=$(date +%s%3N)

          IFS=',' read -ra MODEL_ARRAY <<< "$MODELS"
          for model in "${MODEL_ARRAY[@]}"; do
            echo "----------------------------------------"
            echo "Testing: $model (userspace)"
            echo "----------------------------------------"
            python3 scripts/test-model.py "$model" \
              --output-dir model-results-userspace \
              --timeout 600 || echo "Warning: $model test had issues"
          done

          TEST_END=$(date +%s%3N)
          TEST_DURATION=$((TEST_END - TEST_START))
          echo "userspace_duration_ms=$TEST_DURATION" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 1: Stop Core (Userspace)"
        run: |
          if [ -f /tmp/core-userspace.pid ]; then
            kill $(cat /tmp/core-userspace.pid) 2>/dev/null || true
            rm /tmp/core-userspace.pid
          fi
          sleep 2

      # ========================================================================
      # PHASE 2: KERNEL TESTS (With kernel module)
      # ========================================================================
      - name: "Phase 2: Load kernel module"
        run: |
          echo "========================================"
          echo "PHASE 2: KERNEL TESTS"
          echo "========================================"

          # Find kernel module
          MODULE_PATH=""
          for path in "/lib/modules/$(uname -r)/extra/mlos-ml.ko" \
                      "/opt/mlos/kernel/mlos-ml.ko" \
                      "$HOME/mlos-ml.ko"; do
            if [ -f "$path" ]; then
              MODULE_PATH="$path"
              break
            fi
          done

          if [ -z "$MODULE_PATH" ]; then
            echo "ERROR: mlos-ml.ko not found"
            exit 1
          fi

          echo "Found kernel module: $MODULE_PATH"

          # Set parameters based on mode
          case "${KERNEL_MODE}" in
            basic)
              PARAMS="tmm_enabled=1 scheduler_enabled=0 gpu_enabled=0"
              ;;
            scheduler)
              PARAMS="tmm_enabled=1 scheduler_enabled=1 gpu_enabled=0"
              ;;
            full)
              PARAMS="tmm_enabled=1 scheduler_enabled=1 gpu_enabled=1"
              ;;
          esac

          echo "Loading module with params: $PARAMS"
          sudo insmod "$MODULE_PATH" $PARAMS

          if lsmod | grep -q "mlos_ml\|mlos-ml"; then
            echo "Kernel module loaded successfully"
            lsmod | grep mlos
          else
            echo "ERROR: Failed to load kernel module"
            exit 1
          fi

      - name: "Phase 2: Start Core (Kernel Mode)"
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)

          echo "Starting Core with kernel module..."
          nohup "$BINARY" > /tmp/core-kernel.log 2>&1 &
          echo $! > /tmp/core-kernel.pid

          # Wait for startup
          for i in {1..30}; do
            if curl -s http://localhost:8080/health > /dev/null 2>&1; then
              echo "Core started successfully (kernel mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 2: Run E2E tests (Kernel)"
        id: test-kernel
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-kernel
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (kernel): $MODELS"
          TEST_START=$(date +%s%3N)

          IFS=',' read -ra MODEL_ARRAY <<< "$MODELS"
          for model in "${MODEL_ARRAY[@]}"; do
            echo "----------------------------------------"
            echo "Testing: $model (kernel mode)"
            echo "----------------------------------------"
            python3 scripts/test-model.py "$model" \
              --output-dir model-results-kernel \
              --timeout 600 || echo "Warning: $model test had issues"
          done

          TEST_END=$(date +%s%3N)
          TEST_DURATION=$((TEST_END - TEST_START))
          echo "kernel_duration_ms=$TEST_DURATION" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 2: Stop Core (Kernel)"
        run: |
          if [ -f /tmp/core-kernel.pid ]; then
            kill $(cat /tmp/core-kernel.pid) 2>/dev/null || true
            rm /tmp/core-kernel.pid
          fi

      - name: "Phase 2: Unload kernel module"
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          echo "Kernel module unloaded"

      # ========================================================================
      # GENERATE UNIFIED REPORT
      # ========================================================================
      - name: Generate unified comparison report
        if: always()
        env:
          OS_NAME: ${{ steps.hardware.outputs.os_name }}
          KERNEL_VERSION: ${{ steps.hardware.outputs.kernel_version }}
          CPU_MODEL: ${{ steps.hardware.outputs.cpu_model }}
          CPU_CORES: ${{ steps.hardware.outputs.cpu_cores }}
          MEMORY_GB: ${{ steps.hardware.outputs.memory_gb }}
          GPU_NAME: ${{ steps.hardware.outputs.gpu_name }}
          GPU_COUNT: ${{ steps.hardware.outputs.gpu_count }}
        run: |
          python3 << 'REPORT_SCRIPT'
          import json
          import os
          from pathlib import Path
          from datetime import datetime

          def get_inference_time(data):
              """Extract inference time from result JSON."""
              phases = data.get("phases", {})
              if "inference_large" in phases:
                  return phases["inference_large"].get("time_ms", 0)
              elif "inference_small" in phases:
                  return phases["inference_small"].get("time_ms", 0)
              return data.get("inference_time_ms", 0)

          def get_install_time(data):
              """Extract install time from result JSON."""
              phases = data.get("phases", {})
              if "install" in phases:
                  return phases["install"].get("time_ms", 0)
              return data.get("install_time_ms", 0)

          # Hardware info
          hardware = {
              "os": os.environ.get("OS_NAME", "Linux"),
              "kernel_version": os.environ.get("KERNEL_VERSION", "unknown"),
              "cpu_model": os.environ.get("CPU_MODEL", "unknown"),
              "cpu_cores": int(os.environ.get("CPU_CORES", 0)),
              "memory_gb": int(os.environ.get("MEMORY_GB", 0)),
              "gpu_name": os.environ.get("GPU_NAME", "None"),
              "gpu_count": int(os.environ.get("GPU_COUNT", 0))
          }

          # Collect userspace results
          userspace_results = {}
          user_dir = Path("model-results-userspace")
          if user_dir.exists():
              for f in user_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      userspace_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                          "install_ms": get_install_time(data)
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Collect kernel results
          kernel_results = {}
          kernel_dir = Path("model-results-kernel")
          if kernel_dir.exists():
              for f in kernel_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      kernel_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                          "install_ms": get_install_time(data)
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Calculate speedups
          speedup = {}
          for model in kernel_results:
              if model in userspace_results:
                  k_inf = kernel_results[model]["inference_ms"]
                  u_inf = userspace_results[model]["inference_ms"]
                  if k_inf > 0 and u_inf > 0:
                      # speedup = userspace / kernel (>1 means kernel faster)
                      speedup[model] = round(u_inf / k_inf, 2)

          # Calculate average speedup
          avg_speedup = 0
          if speedup:
              valid_speedups = [v for v in speedup.values() if v > 0]
              if valid_speedups:
                  avg_speedup = round(sum(valid_speedups) / len(valid_speedups), 2)

          # Build metrics
          metrics = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "workflow": "unified",
              "axon_version": os.environ.get("AXON_VERSION", "unknown"),
              "core_version": os.environ.get("CORE_VERSION", "unknown"),
              "hardware": hardware,
              "kernel_comparison": {
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "kernel_mode": os.environ.get("KERNEL_MODE", "scheduler"),
                  "kernel_module_version": "1.0.0",
                  "models_tested": len(kernel_results),
                  "comparison_enabled": True,
                  "hardware": hardware,
                  "kernel_results": kernel_results,
                  "userspace_results": userspace_results,
                  "speedup": speedup,
                  "average_speedup": avg_speedup
              },
              "resources": {
                  "kernel_data_available": True
              }
          }

          # Add model results for main report section
          all_models = set(list(kernel_results.keys()) + list(userspace_results.keys()))
          models_section = {}
          for model in all_models:
              # Use kernel results as primary (since this is kernel-focused workflow)
              if model in kernel_results:
                  models_section[model] = kernel_results[model]
              elif model in userspace_results:
                  models_section[model] = userspace_results[model]

          metrics["models"] = models_section
          metrics["summary"] = {
              "total_models": len(all_models),
              "kernel_faster_count": len([s for s in speedup.values() if s > 1.0]),
              "userspace_faster_count": len([s for s in speedup.values() if s < 1.0]),
              "same_count": len([s for s in speedup.values() if s == 1.0]),
              "average_speedup": avg_speedup
          }

          # Write metrics
          Path("metrics").mkdir(exist_ok=True)
          with open("metrics/latest.json", "w") as f:
              json.dump(metrics, f, indent=2)

          with open("metrics/kernel-comparison.json", "w") as f:
              json.dump(metrics["kernel_comparison"], f, indent=2)

          # Print summary
          print("\n" + "=" * 60)
          print("UNIFIED E2E TEST RESULTS")
          print("=" * 60)
          print(f"Hardware: {hardware['cpu_model']} ({hardware['cpu_cores']} cores, {hardware['memory_gb']}GB)")
          print(f"Models tested: {len(all_models)}")
          print(f"Kernel faster: {metrics['summary']['kernel_faster_count']}")
          print(f"Userspace faster: {metrics['summary']['userspace_faster_count']}")
          print(f"Average speedup: {avg_speedup}x")
          print()
          print("Per-model comparison:")
          print("-" * 60)
          for model in sorted(speedup.keys()):
              k = kernel_results.get(model, {}).get("inference_ms", 0)
              u = userspace_results.get(model, {}).get("inference_ms", 0)
              s = speedup[model]
              winner = "KERNEL" if s > 1 else "USERSPACE" if s < 1 else "SAME"
              print(f"  {model}: K={k}ms U={u}ms -> {s}x ({winner})")

          REPORT_SCRIPT

      - name: Generate HTML report
        if: always()
        run: |
          python3 report/render.py \
            --metrics metrics/latest.json \
            --template report/template.html \
            --output output/index.html

      # ========================================================================
      # UPLOAD ARTIFACTS & DEPLOY
      # ========================================================================
      - name: Upload metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-metrics-${{ github.run_number }}
          path: metrics/
          retention-days: 90

      - name: Upload model results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-results-${{ github.run_number }}
          path: |
            model-results-userspace/
            model-results-kernel/
          retention-days: 30

      - name: Setup Pages
        if: always()
        uses: actions/configure-pages@v4

      - name: Prepare Pages artifact
        if: always()
        run: |
          mkdir -p _site
          cp output/index.html _site/
          cp -r metrics _site/

          # Copy model results for drill-down
          mkdir -p _site/results
          cp -r model-results-userspace _site/results/ 2>/dev/null || true
          cp -r model-results-kernel _site/results/ 2>/dev/null || true

      - name: Upload Pages artifact
        if: always()
        uses: actions/upload-pages-artifact@v3
        with:
          path: _site

      - name: Deploy to GitHub Pages
        if: always()
        uses: actions/deploy-pages@v4

      - name: Summary
        if: always()
        run: |
          echo "## Unified E2E Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f metrics/latest.json ]; then
            python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import json
          with open("metrics/latest.json") as f:
              data = json.load(f)

          kc = data.get("kernel_comparison", {})
          hw = data.get("hardware", {})
          summary = data.get("summary", {})

          print(f"**Hardware:** {hw.get('cpu_model', 'N/A')} ({hw.get('cpu_cores', 0)} cores)")
          print(f"**Memory:** {hw.get('memory_gb', 0)} GB")
          print()
          print(f"| Metric | Value |")
          print(f"|--------|-------|")
          print(f"| Models Tested | {summary.get('total_models', 0)} |")
          print(f"| Kernel Faster | {summary.get('kernel_faster_count', 0)} |")
          print(f"| Userspace Faster | {summary.get('userspace_faster_count', 0)} |")
          print(f"| Average Speedup | {summary.get('average_speedup', 0)}x |")
          print()
          print("### Per-Model Results")
          print()
          print("| Model | Kernel (ms) | Userspace (ms) | Speedup |")
          print("|-------|-------------|----------------|---------|")

          speedup = kc.get("speedup", {})
          kernel_results = kc.get("kernel_results", {})
          userspace_results = kc.get("userspace_results", {})

          for model in sorted(speedup.keys()):
              k = kernel_results.get(model, {}).get("inference_ms", 0)
              u = userspace_results.get(model, {}).get("inference_ms", 0)
              s = speedup[model]
              emoji = "+" if s > 1 else "-" if s < 1 else ""
              print(f"| {model} | {k:.0f} | {u:.0f} | {emoji}{((s-1)*100):.1f}% |")
          EOF
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Report:** https://mlos-foundation.github.io/system-test/" >> $GITHUB_STEP_SUMMARY

      # ========================================================================
      # CLEANUP
      # ========================================================================
      - name: Post-run cleanup
        if: always()
        run: |
          # Ensure kernel module is unloaded
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # Stop any Core processes
          pkill -f mlos_core 2>/dev/null || true

          # Clean up large files
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true
          rm -rf ~/mlos-core/models 2>/dev/null || true

          echo "Cleanup complete"
