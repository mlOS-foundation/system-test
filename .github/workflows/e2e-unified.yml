name: E2E Unified Pipeline (Kernel + Userspace)

# Per-model isolation: Tests each model with BOTH userspace and kernel back-to-back
# ensuring identical system conditions for accurate comparison.

on:
  schedule:
    - cron: '0 6 * * 0'  # Weekly on Sunday at 6 AM UTC
  workflow_dispatch:
    inputs:
      axon_version:
        description: 'Axon version to test'
        required: false
        default: 'v3.1.9'
      core_version:
        description: 'Core version to test'
        required: false
        default: '7.0.0'
      kernel_mode:
        description: 'Kernel module mode'
        required: true
        type: choice
        options:
          - basic
          - scheduler
          - full
        default: 'scheduler'
      models:
        description: 'Models to test (space-separated, empty = all enabled)'
        required: false
        default: ''
      reset_history:
        description: 'Reset historical metrics (start fresh statistics)'
        required: false
        type: boolean
        default: false
      track_history:
        description: 'Add this run to historical metrics'
        required: false
        type: boolean
        default: true

permissions:
  contents: read
  actions: read
  packages: read

jobs:
  unified-e2e-test:
    name: E2E Test (Unified - Kernel + Userspace)
    runs-on: [self-hosted, linux, kernel-capable]

    steps:
      # ========================================================================
      # SETUP & CLEANUP
      # ========================================================================
      - name: Clean up from previous runs
        run: |
          echo "========================================"
          echo "PURGING ALL STATE FROM PREVIOUS RUNS"
          echo "========================================"

          # Stop any running Core processes
          echo "Stopping any running Core processes..."
          pkill -f mlos_core 2>/dev/null || true
          sleep 2

          # Unload kernel module if loaded
          echo "Unloading kernel module..."
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # AGGRESSIVE cache cleanup - remove ALL axon and huggingface caches
          echo "Purging ALL caches..."
          rm -rf ~/.axon 2>/dev/null || true
          rm -rf ~/.cache/huggingface 2>/dev/null || true
          rm -rf ~/.cache/torch 2>/dev/null || true
          rm -rf ~/mlos-core 2>/dev/null || true
          rm -rf /tmp/axon* /tmp/model* /tmp/onnx* /tmp/hf* /tmp/torch* 2>/dev/null || true

          # Clean up test directories
          echo "Cleaning test directories..."
          rm -rf model-results-userspace model-results-kernel model-results metrics output 2>/dev/null || true

          # Clean up downloaded kernel modules from previous runs
          echo "Cleaning downloaded modules..."
          rm -rf ~/mlos-ml-*.ko 2>/dev/null || true

          # Clear pip cache
          pip cache purge 2>/dev/null || true

          # Sync filesystem
          sync

          # Drop kernel caches for consistent initial state
          echo "Dropping kernel caches for consistent benchmark baseline..."
          echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true

          echo "Disk space before run:"
          df -h /
          echo ""
          echo "Memory available:"
          free -m

      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install pyyaml pillow numpy transformers torch torchvision jinja2 requests
          sudo apt-get update && sudo apt-get install -y curl tar jq

      - name: Download golden images
        run: |
          chmod +x scripts/download-golden-images.sh
          ./scripts/download-golden-images.sh

      - name: Get enabled models
        id: get-models
        run: |
          MODELS_INPUT=$(echo "${{ github.event.inputs.models }}" | tr ',' ' ' | xargs)

          if [ -n "$MODELS_INPUT" ]; then
            MODELS="$MODELS_INPUT"
          else
            MODELS=$(python3 -c "
          import yaml
          with open('config/models.yaml') as f:
              config = yaml.safe_load(f)
          models = [name for name, m in config.get('models', {}).items() if m.get('enabled', False)]
          print(' '.join(models))
          ")
          fi

          echo "models=$MODELS" >> $GITHUB_OUTPUT
          echo "Testing models: $MODELS"

      - name: Collect hardware info
        id: hardware
        run: |
          echo "os_name=$(lsb_release -ds 2>/dev/null || cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)" >> $GITHUB_OUTPUT
          echo "kernel_version=$(uname -r)" >> $GITHUB_OUTPUT
          echo "cpu_model=$(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> $GITHUB_OUTPUT
          echo "cpu_cores=$(nproc)" >> $GITHUB_OUTPUT
          echo "memory_gb=$(free -g | awk '/^Mem:/{print $2}')" >> $GITHUB_OUTPUT

          if command -v nvidia-smi &> /dev/null; then
            GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
          else
            GPU_NAME="None (CPU only)"
          fi
          echo "gpu_name=$GPU_NAME" >> $GITHUB_OUTPUT

          echo "Hardware Info:"
          echo "  OS: $(lsb_release -ds 2>/dev/null || echo 'Linux')"
          echo "  CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
          echo "  Cores: $(nproc)"
          echo "  Memory: $(free -g | awk '/^Mem:/{print $2}') GB"
          echo "  GPU: $GPU_NAME"

      # ========================================================================
      # DOWNLOAD RELEASES
      # ========================================================================
      - name: Download Axon
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/.local/bin
          curl -L -f -o /tmp/axon.tar.gz \
            "https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/axon_${VERSION#v}_linux_${ARCH}.tar.gz"
          tar -xzf /tmp/axon.tar.gz -C ~/.local/bin
          chmod +x ~/.local/bin/axon
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          ~/.local/bin/axon version

      - name: Download converter image
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          VERSION_NO_V=${VERSION#v}
          OS="linux"
          ARCH=$(uname -m)
          case $ARCH in
            x86_64) ARCH="amd64" ;;
            aarch64) ARCH="arm64" ;;
          esac

          CONVERTER="axon-converter-${VERSION_NO_V}-${OS}-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/${CONVERTER}"
          IMAGE_TAG="ghcr.io/mlos-foundation/axon-converter:${VERSION_NO_V}"

          echo "Downloading converter image..."
          if curl -L -f -o "/tmp/${CONVERTER}" "$URL" 2>/dev/null; then
            docker load -i "/tmp/${CONVERTER}"
            docker tag "$IMAGE_TAG" "ghcr.io/mlos-foundation/axon-converter:latest" || true
            echo "Converter image loaded"
          else
            echo "WARNING: Converter image not available"
          fi

      - name: Download Core
        run: |
          VERSION="${{ github.event.inputs.core_version || '7.0.0-beta' }}"
          VERSION_NO_V="${VERSION#v}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/mlos-core

          ARTIFACT="mlos-core_${VERSION_NO_V}_linux-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/core-releases/releases/download/v${VERSION_NO_V}/${ARTIFACT}"

          echo "Downloading Core from: $URL"
          curl -L -f -o /tmp/$ARTIFACT "$URL"
          tar -xzf /tmp/$ARTIFACT -C ~/mlos-core

          # Download ONNX Runtime
          ONNX_ARCH="x64"; [ "$(uname -m)" = "aarch64" ] && ONNX_ARCH="aarch64"
          curl -L -f -o /tmp/onnx.tgz \
            "https://github.com/microsoft/onnxruntime/releases/download/v1.18.0/onnxruntime-linux-${ONNX_ARCH}-1.18.0.tgz"

          rm -rf ~/mlos-core/build/onnxruntime 2>/dev/null || true
          mkdir -p ~/mlos-core/build
          tar -xzf /tmp/onnx.tgz -C ~/mlos-core/build
          mv ~/mlos-core/build/onnxruntime-* ~/mlos-core/build/onnxruntime

          echo "Core downloaded and extracted"
          ls -la ~/mlos-core/

      - name: Download kernel module
        id: kernel-check
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          KERNEL_MODULE_VERSION="${{ github.event.inputs.core_version || '7.0.0-beta' }}"
          echo "kernel_module_version=$KERNEL_MODULE_VERSION" >> $GITHUB_OUTPUT

          echo "Runner kernel: $(uname -r)"
          LINUX_KERNEL=$(uname -r)

          # Map to supported kernel versions
          KERNEL_MAJOR=$(echo "$LINUX_KERNEL" | cut -d. -f1,2)
          case "$KERNEL_MAJOR" in
            5.15*) KERNEL_SUFFIX="5.15" ;;
            6.1*)  KERNEL_SUFFIX="6.1" ;;
            6.5*)  KERNEL_SUFFIX="6.5" ;;
            6.8*)  KERNEL_SUFFIX="6.8" ;;
            *)
              echo "Unsupported kernel version: $KERNEL_MAJOR"
              KERNEL_SUFFIX="6.8"
              ;;
          esac

          MODULE_FILE="mlos-ml-v${KERNEL_MODULE_VERSION}-linux-${KERNEL_SUFFIX}.ko"
          DOWNLOAD_PATH="$HOME/${MODULE_FILE}"

          echo "Downloading kernel module: $MODULE_FILE"
          if gh release download "v${KERNEL_MODULE_VERSION}" \
              --repo mlOS-foundation/mlos-linux-kernel \
              --pattern "$MODULE_FILE" \
              --dir "$HOME" 2>/dev/null; then
            MODULE_PATH="$DOWNLOAD_PATH"
            echo "✅ Downloaded module: $MODULE_PATH"
          else
            echo "⚠️  Could not download module from releases, trying pre-installed..."
            for path in "/lib/modules/${LINUX_KERNEL}/extra/mlos-ml.ko" \
                        "/lib/modules/${LINUX_KERNEL}/extra/mlos_ml.ko" \
                        "/opt/mlos/kernel/mlos-ml.ko" \
                        "/opt/mlos/kernel/mlos_ml.ko"; do
              if [ -f "$path" ]; then
                MODULE_PATH="$path"
                echo "Found pre-installed module: $MODULE_PATH"
                break
              fi
            done
          fi

          if [ -z "$MODULE_PATH" ] || [ ! -f "$MODULE_PATH" ]; then
            echo "❌ ERROR: No kernel module found!"
            exit 1
          fi

          echo "module_path=$MODULE_PATH" >> $GITHUB_OUTPUT

      # ========================================================================
      # PER-MODEL ISOLATION TESTS
      # Each model is tested with BOTH userspace and kernel back-to-back
      # ensuring identical system conditions for accurate comparison
      # ========================================================================
      - name: "Run Per-Model Isolation Tests"
        id: test-models
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          MODULE_PATH="${{ steps.kernel-check.outputs.module_path }}"
          KERNEL_MODE="${{ github.event.inputs.kernel_mode || 'scheduler' }}"

          # Compute kernel params
          KERNEL_PARAMS="debug_level=2"
          case "$KERNEL_MODE" in
            basic)
              KERNEL_PARAMS="$KERNEL_PARAMS enable_tmm=1 enable_scheduler=0 enable_gpu_manager=0"
              ;;
            scheduler)
              KERNEL_PARAMS="$KERNEL_PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=0"
              ;;
            full)
              KERNEL_PARAMS="$KERNEL_PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=1"
              ;;
          esac

          # Setup Core binary path
          cd ~/mlos-core
          CORE_BINARY=$(find . -name "mlos_core" -type f -executable | head -1)
          chmod +x "$CORE_BINARY"
          BINARY_DIR=$(dirname "$CORE_BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"
          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"
          cd - > /dev/null

          mkdir -p model-results-userspace model-results-kernel
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "========================================"
          echo "PER-MODEL ISOLATION TEST"
          echo "========================================"
          echo "Each model tested: userspace → kernel (back-to-back)"
          echo "Full system reset between models"
          echo "========================================"

          TEST_START=$(date +%s%3N)
          MODEL_COUNT=0
          TOTAL_MODELS=$(echo "$MODELS" | wc -w)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            MODEL_COUNT=$((MODEL_COUNT + 1))

            echo ""
            echo "╔════════════════════════════════════════════════════════════╗"
            echo "║  MODEL $MODEL_COUNT/$TOTAL_MODELS: $model"
            echo "╚════════════════════════════════════════════════════════════╝"

            # ================================================================
            # STEP 1: FRESH SYSTEM STATE
            # ================================================================
            echo "--- Step 1: Fresh system state ---"

            # Ensure kernel module is NOT loaded
            sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

            # Stop any Core processes
            pkill -f mlos_core 2>/dev/null || true
            sleep 1

            # Clean temp files
            rm -rf /tmp/inference_* /tmp/warmup_* 2>/dev/null || true

            # Sync and drop caches
            sync
            echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true
            sleep 1

            echo "Memory state (fresh):"
            free -m | head -2

            # ================================================================
            # STEP 2: USERSPACE TEST
            # ================================================================
            echo ""
            echo "--- Step 2: Userspace test for $model ---"

            # Start Core in userspace mode
            cd ~/mlos-core
            nohup $CORE_BINARY --http-port 8080 > /tmp/core-userspace.log 2>&1 &
            CORE_PID=$!
            cd - > /dev/null

            # Wait for Core to be ready
            for i in {1..30}; do
              if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
                echo "Core started (userspace mode)"
                break
              fi
              sleep 1
            done

            # Run userspace test
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-userspace \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model userspace test had issues"

            # Stop Core
            kill $CORE_PID 2>/dev/null || true
            pkill -f mlos_core 2>/dev/null || true
            sleep 1

            # ================================================================
            # STEP 3: RESET FOR KERNEL TEST
            # ================================================================
            echo ""
            echo "--- Step 3: Reset for kernel test ---"

            # Clean temp files (keep model cache)
            rm -rf /tmp/inference_* /tmp/warmup_* 2>/dev/null || true
            sync
            echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true
            sleep 1

            echo "Memory state (reset for kernel):"
            free -m | head -2

            # ================================================================
            # STEP 4: KERNEL TEST
            # ================================================================
            echo ""
            echo "--- Step 4: Kernel test for $model ---"

            # Load kernel module
            echo "Loading kernel module..."
            sudo insmod "$MODULE_PATH" $KERNEL_PARAMS
            sleep 1

            if lsmod | grep -q "mlos"; then
              echo "Kernel module loaded"
            else
              echo "ERROR: Failed to load kernel module"
              continue
            fi

            # Start Core in kernel mode
            cd ~/mlos-core
            nohup $CORE_BINARY --http-port 8080 > /tmp/core-kernel.log 2>&1 &
            CORE_PID=$!
            cd - > /dev/null

            # Wait for Core to be ready
            for i in {1..30}; do
              if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
                echo "Core started (kernel mode)"
                break
              fi
              sleep 1
            done

            # Run kernel test
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-kernel \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model kernel test had issues"

            # Stop Core
            kill $CORE_PID 2>/dev/null || true
            pkill -f mlos_core 2>/dev/null || true
            sleep 1

            # Unload kernel module
            sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
            sleep 1

            echo ""
            echo "✅ $model completed (userspace + kernel)"
            echo "----------------------------------------"
          done

      # ================================================================
      # CONCURRENT INFERENCE TESTS (Kernel Scheduler Showcase)
      # ================================================================
      - name: "Run Concurrent Inference Tests"
        id: concurrent-tests
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          MODULE_PATH="${{ steps.kernel-check.outputs.module_path }}"
          KERNEL_MODE="${{ github.event.inputs.kernel_mode || 'scheduler' }}"
          CONCURRENCY=8
          ITERATIONS=3

          # Compute kernel params
          KERNEL_PARAMS="debug_level=2"
          case "$KERNEL_MODE" in
            basic)
              KERNEL_PARAMS="$KERNEL_PARAMS enable_tmm=1 enable_scheduler=0 enable_gpu_manager=0"
              ;;
            scheduler)
              KERNEL_PARAMS="$KERNEL_PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=0"
              ;;
            full)
              KERNEL_PARAMS="$KERNEL_PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=1"
              ;;
          esac

          # Setup Core binary path
          cd ~/mlos-core
          CORE_BINARY=$(find . -name "mlos_core" -type f -executable | head -1)
          chmod +x "$CORE_BINARY"
          BINARY_DIR=$(dirname "$CORE_BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"
          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"
          cd - > /dev/null

          mkdir -p concurrent-results-userspace concurrent-results-kernel
          chmod +x scripts/test-concurrent-inference.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo ""
          echo "╔════════════════════════════════════════════════════════════╗"
          echo "║  CONCURRENT INFERENCE TESTS                                ║"
          echo "║  Testing ${CONCURRENCY} parallel requests per model        ║"
          echo "║  This showcases kernel scheduler benefits                  ║"
          echo "╚════════════════════════════════════════════════════════════╝"

          # Select representative models for concurrent testing (to save time)
          # Use 5-6 models across different categories
          CONCURRENT_MODELS=""
          for model in bert-base-uncased gpt2 t5-small resnet50 mobilenet; do
            if echo "$MODELS" | grep -qw "$model"; then
              CONCURRENT_MODELS="$CONCURRENT_MODELS $model"
            fi
          done

          # Fallback: use first 5 models if none of the representative models are available
          if [ -z "$(echo $CONCURRENT_MODELS | xargs)" ]; then
            CONCURRENT_MODELS=$(echo "$MODELS" | tr ' ' '\n' | head -5 | tr '\n' ' ')
          fi

          echo "Testing concurrent inference for: $CONCURRENT_MODELS"

          for model in $CONCURRENT_MODELS; do
            [ -z "$model" ] && continue

            echo ""
            echo "── Concurrent test: $model ──"

            # ================================================================
            # USERSPACE CONCURRENT TEST
            # ================================================================
            echo "Starting userspace concurrent test..."

            # Ensure clean state
            sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
            pkill -f mlos_core 2>/dev/null || true
            sleep 1
            sync
            echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true

            # Start Core (userspace mode)
            cd ~/mlos-core
            nohup $CORE_BINARY --http-port 8080 > /tmp/core-concurrent-user.log 2>&1 &
            CORE_PID=$!
            cd - > /dev/null

            # Wait for ready
            for i in {1..30}; do
              if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
                break
              fi
              sleep 1
            done

            # Run concurrent test
            ./scripts/test-concurrent-inference.sh "$model" \
              --concurrency $CONCURRENCY \
              --iterations $ITERATIONS \
              --output-dir concurrent-results-userspace \
              --core-url http://127.0.0.1:8080 \
              || echo "Warning: $model userspace concurrent test had issues"

            # Stop Core
            kill $CORE_PID 2>/dev/null || true
            pkill -f mlos_core 2>/dev/null || true
            sleep 1

            # ================================================================
            # KERNEL CONCURRENT TEST
            # ================================================================
            echo "Starting kernel concurrent test..."

            sync
            echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true

            # Load kernel module
            sudo insmod "$MODULE_PATH" $KERNEL_PARAMS
            sleep 1

            # Start Core (kernel mode)
            cd ~/mlos-core
            nohup $CORE_BINARY --http-port 8080 > /tmp/core-concurrent-kernel.log 2>&1 &
            CORE_PID=$!
            cd - > /dev/null

            # Wait for ready
            for i in {1..30}; do
              if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
                break
              fi
              sleep 1
            done

            # Run concurrent test
            ./scripts/test-concurrent-inference.sh "$model" \
              --concurrency $CONCURRENCY \
              --iterations $ITERATIONS \
              --output-dir concurrent-results-kernel \
              --core-url http://127.0.0.1:8080 \
              || echo "Warning: $model kernel concurrent test had issues"

            # Stop Core and unload kernel
            kill $CORE_PID 2>/dev/null || true
            pkill -f mlos_core 2>/dev/null || true
            sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
            sleep 1

            echo "✅ $model concurrent tests complete"
          done

          echo ""
          echo "========================================"
          echo "CONCURRENT INFERENCE TESTS COMPLETE"
          echo "========================================"
        continue-on-error: true

      # ========================================================================
      # GENERATE UNIFIED REPORT
      # ========================================================================
      - name: Collect hardware and timing info
        if: always()
        run: |
          mkdir -p model-results

          cat > model-results/hardware-info.json << EOF
          {
            "os": "$(uname -s)",
            "os_version": "$(cat /etc/os-release 2>/dev/null | grep PRETTY_NAME | cut -d'"' -f2 || uname -r)",
            "arch": "$(uname -m)",
            "cpu_model": "$(lscpu 2>/dev/null | grep 'Model name' | cut -d':' -f2 | xargs || echo 'Unknown')",
            "cpu_cores": $(nproc --all 2>/dev/null || echo 2),
            "cpu_threads": $(nproc 2>/dev/null || echo 2),
            "memory_gb": $(free -g 2>/dev/null | awk '/^Mem:/{print $2}' || echo 7),
            "gpu_count": $(nvidia-smi -L 2>/dev/null | wc -l || echo 0),
            "gpu_name": "$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -1 || echo 'None')",
            "gpu_memory": "$(nvidia-smi --query-gpu=memory.total --format=csv,noheader 2>/dev/null | head -1 || echo 'N/A')",
            "disk_total": "$(df -h / | tail -1 | awk '{print $2}')",
            "disk_available": "$(df -h / | tail -1 | awk '{print $4}')"
          }
          EOF

          cat > model-results/timings-info.json << EOF
          {
            "axon_download_ms": 0,
            "core_download_ms": 0,
            "core_startup_ms": 0
          }
          EOF

          cat > model-results/resources-info.json << EOF
          {
            "core_idle_cpu": 0,
            "core_idle_mem_mb": 0,
            "core_load_cpu_avg": 0,
            "core_load_cpu_max": 0,
            "core_load_mem_avg_mb": 0,
            "core_load_mem_max_mb": 0,
            "axon_cpu": 0,
            "axon_mem_mb": 0,
            "gpu_status": "Not used (CPU-only inference)",
            "kernel_mode": "${{ github.event.inputs.kernel_mode || 'scheduler' }}",
            "kernel_module_loaded": true,
            "test_mode": "per_model_isolation"
          }
          EOF

          cp model-results-userspace/*-result.json model-results/ 2>/dev/null || true

      - name: Aggregate userspace results
        if: always()
        env:
          AXON_VERSION: ${{ github.event.inputs.axon_version || 'v3.1.9' }}
          CORE_VERSION: ${{ github.event.inputs.core_version || '7.0.0-beta' }}
        run: |
          mkdir -p output metrics

          python3 scripts/aggregate-results.py \
            --results-dir model-results \
            --output metrics/latest.json \
            --markdown output/RESULTS.md \
            --hardware-info model-results/hardware-info.json \
            --timings-info model-results/timings-info.json \
            --resources-info model-results/resources-info.json \
            --github-summary || echo "Aggregation completed"

      - name: Generate kernel comparison
        if: always()
        env:
          KERNEL_MODE: ${{ github.event.inputs.kernel_mode || 'scheduler' }}
          KERNEL_MODULE_VERSION: ${{ steps.kernel-check.outputs.kernel_module_version || '7.0.0-beta' }}
        run: |
          python3 << 'KERNEL_SCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime
          import os

          def get_inference_times(data):
              phases = data.get("phases", {})
              small_ms = phases.get("inference_small", {}).get("time_ms", 0)
              large_ms = phases.get("inference_large", {}).get("time_ms", 0)
              if small_ms == 0 and large_ms == 0:
                  legacy = data.get("inference_time_ms", 0)
                  small_ms = legacy
              return small_ms, large_ms

          def get_install_time(data):
              phases = data.get("phases", {})
              return phases.get("install", {}).get("time_ms", 0)

          # Load userspace results
          userspace_results = {}
          user_dir = Path("model-results-userspace")
          if user_dir.exists():
              for f in user_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      small_ms, large_ms = get_inference_times(data)
                      userspace_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_small_ms": small_ms,
                          "inference_large_ms": large_ms,
                          "inference_ms": large_ms if large_ms > 0 else small_ms,
                          "install_ms": get_install_time(data),
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Load kernel results
          kernel_results = {}
          kernel_dir = Path("model-results-kernel")
          if kernel_dir.exists():
              for f in kernel_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      small_ms, large_ms = get_inference_times(data)
                      kernel_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_small_ms": small_ms,
                          "inference_large_ms": large_ms,
                          "inference_ms": large_ms if large_ms > 0 else small_ms,
                          "install_ms": get_install_time(data),
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Calculate speedups
          speedup = {}
          speedup_small = {}
          speedup_large = {}

          for model in kernel_results:
              if model in userspace_results:
                  kr = kernel_results[model]
                  ur = userspace_results[model]

                  k_small = kr.get("inference_small_ms", 0)
                  u_small = ur.get("inference_small_ms", 0)
                  if k_small > 0 and u_small > 0:
                      speedup_small[model] = round(u_small / k_small, 2)

                  k_large = kr.get("inference_large_ms", 0)
                  u_large = ur.get("inference_large_ms", 0)
                  if k_large > 0 and u_large > 0:
                      speedup_large[model] = round(u_large / k_large, 2)

                  k_inf = kr.get("inference_ms", 0)
                  u_inf = ur.get("inference_ms", 0)
                  if k_inf > 0 and u_inf > 0:
                      speedup[model] = round(u_inf / k_inf, 2)

          def calc_avg(d):
              valid = [v for v in d.values() if v > 0]
              return round(sum(valid) / len(valid), 2) if valid else 0

          avg_speedup = calc_avg(speedup)
          avg_speedup_small = calc_avg(speedup_small)
          avg_speedup_large = calc_avg(speedup_large)

          # Load concurrent inference results
          concurrent_userspace = {}
          concurrent_kernel = {}
          concurrent_speedup_throughput = {}
          concurrent_speedup_p95 = {}

          concurrent_user_dir = Path("concurrent-results-userspace")
          concurrent_kernel_dir = Path("concurrent-results-kernel")

          if concurrent_user_dir.exists():
              for f in concurrent_user_dir.glob("*-concurrent.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = data.get("model", f.stem.replace("-concurrent", ""))
                      summary = data.get("summary", {})
                      concurrent_userspace[model] = {
                          "throughput_rps": summary.get("avg_throughput_rps", 0),
                          "latency_avg_ms": summary.get("avg_latency_ms", 0),
                          "latency_p95_ms": summary.get("avg_p95_ms", 0),
                      }
                  except Exception as e:
                      print(f"Error reading concurrent userspace {f}: {e}")

          if concurrent_kernel_dir.exists():
              for f in concurrent_kernel_dir.glob("*-concurrent.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = data.get("model", f.stem.replace("-concurrent", ""))
                      summary = data.get("summary", {})
                      concurrent_kernel[model] = {
                          "throughput_rps": summary.get("avg_throughput_rps", 0),
                          "latency_avg_ms": summary.get("avg_latency_ms", 0),
                          "latency_p95_ms": summary.get("avg_p95_ms", 0),
                      }
                  except Exception as e:
                      print(f"Error reading concurrent kernel {f}: {e}")

          # Calculate concurrent speedups
          for model in concurrent_kernel:
              if model in concurrent_userspace:
                  k_tput = concurrent_kernel[model].get("throughput_rps", 0)
                  u_tput = concurrent_userspace[model].get("throughput_rps", 0)
                  if k_tput > 0 and u_tput > 0:
                      concurrent_speedup_throughput[model] = round(k_tput / u_tput, 2)

                  k_p95 = concurrent_kernel[model].get("latency_p95_ms", 0)
                  u_p95 = concurrent_userspace[model].get("latency_p95_ms", 0)
                  if k_p95 > 0 and u_p95 > 0:
                      concurrent_speedup_p95[model] = round(u_p95 / k_p95, 2)

          avg_concurrent_speedup_throughput = calc_avg(concurrent_speedup_throughput)
          avg_concurrent_speedup_p95 = calc_avg(concurrent_speedup_p95)

          # Load hardware info
          hardware = {}
          hw_file = Path("model-results/hardware-info.json")
          if hw_file.exists():
              try:
                  with open(hw_file) as f:
                      hardware = json.load(f)
              except:
                  pass

          kernel_comparison = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "kernel_mode": os.environ.get("KERNEL_MODE", "scheduler"),
              "kernel_module_version": os.environ.get("KERNEL_MODULE_VERSION", "7.0.0-beta"),
              "models_tested": len(set(list(kernel_results.keys()) + list(userspace_results.keys()))),
              "comparison_enabled": True,
              "test_mode": "per_model_isolation",
              "hardware": hardware,
              "kernel_results": kernel_results,
              "userspace_results": userspace_results,
              "speedup": speedup,
              "average_speedup": avg_speedup,
              "speedup_small": speedup_small,
              "speedup_large": speedup_large,
              "average_speedup_small": avg_speedup_small,
              "average_speedup_large": avg_speedup_large,
              # Concurrent inference metrics
              "concurrent_tests": {
                  "models_tested": len(concurrent_speedup_throughput),
                  "concurrency": 8,
                  "userspace_results": concurrent_userspace,
                  "kernel_results": concurrent_kernel,
                  "speedup_throughput": concurrent_speedup_throughput,
                  "speedup_p95_latency": concurrent_speedup_p95,
                  "average_speedup_throughput": avg_concurrent_speedup_throughput,
                  "average_speedup_p95_latency": avg_concurrent_speedup_p95,
              }
          }

          # Merge into metrics
          metrics_file = Path("metrics/latest.json")
          if metrics_file.exists():
              with open(metrics_file) as f:
                  metrics = json.load(f)
              metrics["kernel_comparison"] = kernel_comparison
              with open(metrics_file, "w") as f:
                  json.dump(metrics, f, indent=2)

          with open("metrics/kernel-comparison.json", "w") as f:
              json.dump(kernel_comparison, f, indent=2)

          print(f"\n{'='*60}")
          print(f"KERNEL COMPARISON (Per-Model Isolation)")
          print(f"{'='*60}")
          print(f"  Models tested: {len(speedup)}")
          print(f"  Average speedup: {avg_speedup}x")
          print(f"  Kernel faster: {len([s for s in speedup.values() if s > 1.0])}")
          print(f"  Userspace faster: {len([s for s in speedup.values() if s < 1.0])}")
          print(f"{'='*60}")
          if concurrent_speedup_throughput:
              print(f"\n{'='*60}")
              print(f"CONCURRENT INFERENCE (8 parallel requests)")
              print(f"{'='*60}")
              print(f"  Models tested: {len(concurrent_speedup_throughput)}")
              print(f"  Avg throughput speedup: {avg_concurrent_speedup_throughput}x")
              print(f"  Avg P95 latency improvement: {avg_concurrent_speedup_p95}x")
              for m, s in sorted(concurrent_speedup_throughput.items(), key=lambda x: -x[1]):
                  print(f"    {m}: {s}x throughput")
              print(f"{'='*60}")
          KERNEL_SCRIPT

      - name: Track historical metrics
        if: always() && github.event.inputs.track_history != 'false'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ "${{ github.event.inputs.reset_history }}" != "true" ]; then
            echo "Looking for previous historical data..."

            PREV_RUN=$(gh run list --workflow="e2e-unified.yml" --status=success --limit=10 --json databaseId,conclusion \
              --jq '[.[] | select(.conclusion=="success")] | .[0].databaseId // empty')

            if [ -n "$PREV_RUN" ] && [ "$PREV_RUN" != "${{ github.run_id }}" ]; then
              echo "Downloading history from run $PREV_RUN..."

              ARTIFACT_NAME=$(gh api "repos/${{ github.repository }}/actions/runs/${PREV_RUN}/artifacts" \
                --jq '.artifacts[] | select(.name | startswith("unified-e2e-metrics")) | .name' | head -1)

              if [ -n "$ARTIFACT_NAME" ]; then
                mkdir -p prev_metrics
                gh run download "$PREV_RUN" -n "$ARTIFACT_NAME" -D prev_metrics/ 2>/dev/null || true

                if [ -f prev_metrics/history.json ]; then
                  cp prev_metrics/history.json metrics/history.json
                fi
                rm -rf prev_metrics
              fi
            fi
          fi

          if [ -f metrics/latest.json ]; then
            python3 scripts/historical-metrics.py add \
              --metrics metrics/latest.json \
              --history metrics/history.json

            python3 scripts/historical-metrics.py stats \
              --history metrics/history.json \
              --output metrics/statistics.json

            python3 scripts/historical-metrics.py summary --history metrics/history.json
          fi

      # ========================================================================
      # UPLOAD ARTIFACTS
      # ========================================================================
      - name: Upload metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-metrics-${{ github.run_number }}
          path: metrics/
          retention-days: 90

      - name: Upload model results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-results-${{ github.run_number }}
          path: |
            model-results-userspace/
            model-results-kernel/
            concurrent-results-userspace/
            concurrent-results-kernel/
          retention-days: 30

      - name: Trigger render workflow
        if: success()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Triggering render workflow..."
          for i in $(seq 1 3); do
            if gh workflow run render-report.yml \
                --ref main \
                -f metrics_run_id=${{ github.run_id }} \
                -f metrics_workflow=e2e-unified.yml 2>&1; then
              echo "Render workflow triggered"
              break
            fi
            sleep 5
          done

      - name: Summary
        if: always()
        run: |
          echo "## E2E Per-Model Isolation Test Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Mode**: Per-model isolation (userspace → kernel back-to-back)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f metrics/latest.json ]; then
            python3 -c "
          import json
          with open('metrics/latest.json') as f:
              m = json.load(f)
          kc = m.get('kernel_comparison', {})
          print(f\"### Single Inference Results\")
          print(f\"- Models tested: {kc.get('models_tested', 0)}\")
          print(f\"- Average speedup: {kc.get('average_speedup', 0)}x\")
          speedup = kc.get('speedup', {})
          print(f\"- Kernel faster: {len([s for s in speedup.values() if s > 1.0])}\")
          print(f\"- Userspace faster: {len([s for s in speedup.values() if s < 1.0])}\")

          # Concurrent results
          ct = kc.get('concurrent_tests', {})
          if ct.get('models_tested', 0) > 0:
              print(f\"\")
              print(f\"### Concurrent Inference Results (8 parallel requests)\")
              print(f\"- Models tested: {ct.get('models_tested', 0)}\")
              print(f\"- Avg throughput speedup: {ct.get('average_speedup_throughput', 0)}x\")
              print(f\"- Avg P95 latency improvement: {ct.get('average_speedup_p95_latency', 0)}x\")
              st = ct.get('speedup_throughput', {})
              for m, s in sorted(st.items(), key=lambda x: -x[1])[:5]:
                  print(f\"  - {m}: {s}x\")
          " >> $GITHUB_STEP_SUMMARY
          fi

      # ========================================================================
      # CLEANUP
      # ========================================================================
      - name: Post-run cleanup
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          pkill -f mlos_core 2>/dev/null || true
          rm -rf ~/.cache/huggingface ~/.cache/torch ~/.axon/cache ~/mlos-core 2>/dev/null || true
          rm -rf /tmp/axon* /tmp/model* /tmp/onnx* 2>/dev/null || true
          rm -rf ~/mlos-ml-*.ko 2>/dev/null || true
          sync
          echo "Cleanup complete"
