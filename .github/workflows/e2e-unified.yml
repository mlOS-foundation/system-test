name: E2E Unified Pipeline (Kernel + Userspace)

# Single workflow that runs BOTH userspace and kernel tests on the SAME machine
# for accurate, consistent performance comparison.

on:
  schedule:
    - cron: '0 6 * * 0'  # Weekly on Sunday at 6 AM UTC
  workflow_dispatch:
    inputs:
      axon_version:
        description: 'Axon version to test'
        required: false
        default: 'v3.1.9'
      core_version:
        description: 'Core version to test'
        required: false
        default: '5.0.1-alpha'
      kernel_mode:
        description: 'Kernel module mode'
        required: true
        type: choice
        options:
          - basic
          - scheduler
          - full
        default: 'scheduler'
      models:
        description: 'Models to test (space-separated, empty = all enabled)'
        required: false
        default: ''
      reset_history:
        description: 'Reset historical metrics (start fresh statistics)'
        required: false
        type: boolean
        default: false
      track_history:
        description: 'Add this run to historical metrics'
        required: false
        type: boolean
        default: true

permissions:
  contents: read
  actions: read
  packages: read

jobs:
  unified-e2e-test:
    name: E2E Test (Unified - Kernel + Userspace)
    runs-on: [self-hosted, linux, kernel-capable]

    steps:
      # ========================================================================
      # SETUP & CLEANUP
      # ========================================================================
      - name: Clean up from previous runs
        run: |
          echo "========================================"
          echo "PURGING ALL STATE FROM PREVIOUS RUNS"
          echo "========================================"

          # Stop any running Core processes
          echo "Stopping any running Core processes..."
          pkill -f mlos_core 2>/dev/null || true
          sleep 2

          # Unload kernel module if loaded
          echo "Unloading kernel module..."
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # AGGRESSIVE cache cleanup - remove ALL axon and huggingface caches
          echo "Purging ALL caches..."
          rm -rf ~/.axon 2>/dev/null || true
          rm -rf ~/.cache/huggingface 2>/dev/null || true
          rm -rf ~/.cache/torch 2>/dev/null || true
          rm -rf ~/mlos-core 2>/dev/null || true
          rm -rf /tmp/axon* /tmp/model* /tmp/onnx* /tmp/hf* /tmp/torch* 2>/dev/null || true

          # Clean up test directories
          echo "Cleaning test directories..."
          rm -rf model-results-userspace model-results-kernel model-results metrics output 2>/dev/null || true

          # Clean up downloaded kernel modules from previous runs
          echo "Cleaning downloaded modules..."
          rm -rf ~/mlos-ml-*.ko 2>/dev/null || true

          # Clear pip cache
          pip cache purge 2>/dev/null || true

          # Sync filesystem
          sync

          # Drop kernel caches for consistent initial state
          echo "Dropping kernel caches for consistent benchmark baseline..."
          echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true

          echo "Disk space before run:"
          df -h /
          echo ""
          echo "Memory available:"
          free -m

      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install pyyaml pillow numpy transformers torch torchvision jinja2 requests
          sudo apt-get update && sudo apt-get install -y curl tar jq

      - name: Download golden images
        run: |
          chmod +x scripts/download-golden-images.sh
          ./scripts/download-golden-images.sh

      - name: Get enabled models
        id: get-models
        run: |
          MODELS_INPUT=$(echo "${{ github.event.inputs.models }}" | tr ',' ' ' | xargs)

          if [ -n "$MODELS_INPUT" ]; then
            MODELS="$MODELS_INPUT"
          else
            MODELS=$(python3 -c "
          import yaml
          with open('config/models.yaml') as f:
              config = yaml.safe_load(f)
          models = [name for name, m in config.get('models', {}).items() if m.get('enabled', False)]
          print(' '.join(models))
          ")
          fi

          echo "models=$MODELS" >> $GITHUB_OUTPUT
          echo "Testing models: $MODELS"

      - name: Collect hardware info
        id: hardware
        run: |
          echo "os_name=$(lsb_release -ds 2>/dev/null || cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)" >> $GITHUB_OUTPUT
          echo "kernel_version=$(uname -r)" >> $GITHUB_OUTPUT
          echo "cpu_model=$(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> $GITHUB_OUTPUT
          echo "cpu_cores=$(nproc)" >> $GITHUB_OUTPUT
          echo "memory_gb=$(free -g | awk '/^Mem:/{print $2}')" >> $GITHUB_OUTPUT

          if command -v nvidia-smi &> /dev/null; then
            GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
          else
            GPU_NAME="None (CPU only)"
          fi
          echo "gpu_name=$GPU_NAME" >> $GITHUB_OUTPUT

          echo "Hardware Info:"
          echo "  OS: $(lsb_release -ds 2>/dev/null || echo 'Linux')"
          echo "  CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
          echo "  Cores: $(nproc)"
          echo "  Memory: $(free -g | awk '/^Mem:/{print $2}') GB"
          echo "  GPU: $GPU_NAME"

      # ========================================================================
      # DOWNLOAD RELEASES (using proven patterns from e2e-kernel.yml)
      # ========================================================================
      - name: Download Axon
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/.local/bin
          curl -L -f -o /tmp/axon.tar.gz \
            "https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/axon_${VERSION#v}_linux_${ARCH}.tar.gz"
          tar -xzf /tmp/axon.tar.gz -C ~/.local/bin
          chmod +x ~/.local/bin/axon
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          ~/.local/bin/axon version

      - name: Download converter image
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          VERSION_NO_V=${VERSION#v}
          OS="linux"
          ARCH=$(uname -m)
          case $ARCH in
            x86_64) ARCH="amd64" ;;
            aarch64) ARCH="arm64" ;;
          esac

          CONVERTER="axon-converter-${VERSION_NO_V}-${OS}-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/${CONVERTER}"
          IMAGE_TAG="ghcr.io/mlos-foundation/axon-converter:${VERSION_NO_V}"

          echo "Downloading converter image..."
          if curl -L -f -o "/tmp/${CONVERTER}" "$URL" 2>/dev/null; then
            docker load -i "/tmp/${CONVERTER}"
            docker tag "$IMAGE_TAG" "ghcr.io/mlos-foundation/axon-converter:latest" || true
            echo "Converter image loaded"
          else
            echo "WARNING: Converter image not available"
          fi

      - name: Download Core
        run: |
          VERSION="${{ github.event.inputs.core_version || '5.0.1-alpha' }}"
          VERSION_NO_V="${VERSION#v}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/mlos-core

          # Use public core-releases repo (core is private)
          ARTIFACT="mlos-core_${VERSION_NO_V}_linux-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/core-releases/releases/download/v${VERSION_NO_V}/${ARTIFACT}"

          echo "Downloading Core from: $URL"
          curl -L -f -o /tmp/$ARTIFACT "$URL"
          tar -xzf /tmp/$ARTIFACT -C ~/mlos-core

          # Download ONNX Runtime
          ONNX_ARCH="x64"; [ "$(uname -m)" = "aarch64" ] && ONNX_ARCH="aarch64"
          curl -L -f -o /tmp/onnx.tgz \
            "https://github.com/microsoft/onnxruntime/releases/download/v1.18.0/onnxruntime-linux-${ONNX_ARCH}-1.18.0.tgz"

          rm -rf ~/mlos-core/build/onnxruntime 2>/dev/null || true
          mkdir -p ~/mlos-core/build
          tar -xzf /tmp/onnx.tgz -C ~/mlos-core/build
          mv ~/mlos-core/build/onnxruntime-* ~/mlos-core/build/onnxruntime

          echo "Core downloaded and extracted"
          ls -la ~/mlos-core/

      # ========================================================================
      # PHASE 1: USERSPACE TESTS (No kernel module)
      # ========================================================================
      - name: "Phase 1: Ensure kernel module is NOT loaded"
        run: |
          echo "========================================"
          echo "PHASE 1: USERSPACE TESTS"
          echo "========================================"
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          if lsmod | grep -q "mlos"; then
            echo "ERROR: Kernel module still loaded!"
            exit 1
          fi
          echo "Kernel module confirmed NOT loaded"

      - name: "Phase 1: Start Core (Userspace Mode)"
        id: core-userspace-start
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)
          chmod +x "$BINARY"

          BINARY_DIR=$(dirname "$BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"

          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"

          echo "Starting Core in USERSPACE mode..."
          STARTUP_START=$(date +%s%3N)
          nohup $BINARY --http-port 8080 > /tmp/core-userspace.log 2>&1 &
          echo $! > /tmp/core-userspace.pid

          for i in {1..30}; do
            if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
              STARTUP_END=$(date +%s%3N)
              echo "Core started (userspace mode)"
              echo "core_startup_ms=$((STARTUP_END - STARTUP_START))" >> $GITHUB_OUTPUT
              break
            fi
            sleep 1
          done

          # Capture idle resource usage
          sleep 2
          CORE_PID=$(cat /tmp/core-userspace.pid)
          if [ -n "$CORE_PID" ] && ps -p "$CORE_PID" > /dev/null 2>&1; then
            CORE_IDLE_CPU=$(ps -p "$CORE_PID" -o %cpu= 2>/dev/null | xargs || echo "0")
            CORE_IDLE_MEM_MB=$(ps -p "$CORE_PID" -o rss= 2>/dev/null | awk '{printf "%.0f", $1/1024}' || echo "0")
            echo "core_idle_cpu=$CORE_IDLE_CPU" >> $GITHUB_OUTPUT
            echo "core_idle_mem_mb=$CORE_IDLE_MEM_MB" >> $GITHUB_OUTPUT
            echo "Core idle: CPU=${CORE_IDLE_CPU}%, Memory=${CORE_IDLE_MEM_MB}MB"
          fi

      - name: "Phase 1: Run E2E tests (Userspace)"
        id: test-userspace
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-userspace
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (USERSPACE): $MODELS"
          TEST_START=$(date +%s%3N)

          # Track max resource usage during tests
          MAX_CPU=0
          MAX_MEM=0
          CORE_PID=$(cat /tmp/core-userspace.pid 2>/dev/null)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            echo "----------------------------------------"
            echo "Testing: $model (userspace)"
            echo "----------------------------------------"
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-userspace \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model had issues"

            # Sample resource usage after each model
            if [ -n "$CORE_PID" ] && ps -p "$CORE_PID" > /dev/null 2>&1; then
              CPU=$(ps -p "$CORE_PID" -o %cpu= 2>/dev/null | xargs || echo "0")
              MEM=$(ps -p "$CORE_PID" -o rss= 2>/dev/null | awk '{printf "%.0f", $1/1024}' || echo "0")
              CPU_INT=${CPU%.*}
              MEM_INT=${MEM%.*}
              [ "${CPU_INT:-0}" -gt "${MAX_CPU:-0}" ] && MAX_CPU=$CPU_INT
              [ "${MEM_INT:-0}" -gt "${MAX_MEM:-0}" ] && MAX_MEM=$MEM_INT
            fi
          done

          TEST_END=$(date +%s%3N)
          echo "userspace_duration_ms=$((TEST_END - TEST_START))" >> $GITHUB_OUTPUT
          echo "core_load_cpu_max=$MAX_CPU" >> $GITHUB_OUTPUT
          echo "core_load_mem_max_mb=$MAX_MEM" >> $GITHUB_OUTPUT
          echo "Core load max: CPU=${MAX_CPU}%, Memory=${MAX_MEM}MB"
        continue-on-error: true

      - name: "Phase 1: Stop Core (Userspace)"
        run: |
          if [ -f /tmp/core-userspace.pid ]; then
            kill $(cat /tmp/core-userspace.pid) 2>/dev/null || true
            rm /tmp/core-userspace.pid
          fi
          pkill -f mlos_core 2>/dev/null || true
          sleep 2
          echo "Core (userspace) stopped"

      - name: "Phase 1->2: Clean model cache for fresh kernel tests"
        run: |
          echo "========================================"
          echo "CLEANUP BETWEEN USERSPACE AND KERNEL TESTS"
          echo "========================================"

          # 1. Clean model caches
          echo "Cleaning model caches..."
          rm -rf ~/.axon/cache/models 2>/dev/null || true
          rm -rf ~/.axon/cache 2>/dev/null || true
          rm -rf ~/mlos-core/models 2>/dev/null || true
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true

          # 2. Clean temp files from userspace tests
          echo "Cleaning temp files..."
          rm -rf /tmp/inference_* /tmp/warmup_* /tmp/model_* 2>/dev/null || true

          # 3. Sync filesystem to flush any pending writes
          echo "Syncing filesystem..."
          sync

          # 4. Drop kernel caches (requires sudo) for consistent memory state
          # This ensures kernel tests start with similar memory conditions as userspace
          echo "Dropping kernel caches for consistent benchmark..."
          echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null 2>&1 || true

          # 5. Small delay to let system stabilize
          sleep 2

          # 6. Show memory state
          echo "Memory state before kernel tests:"
          free -m

          echo "Phase cleanup complete"

      # ========================================================================
      # PHASE 2: KERNEL TESTS (With kernel module)
      # ========================================================================
      - name: "Phase 2: Find kernel module"
        id: kernel-check
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "========================================"
          echo "PHASE 2: KERNEL TESTS"
          echo "========================================"

          # Kernel module version should match Core version
          KERNEL_MODULE_VERSION="${{ github.event.inputs.core_version || '5.0.1-alpha' }}"
          echo "kernel_module_version=$KERNEL_MODULE_VERSION" >> $GITHUB_OUTPUT

          # Show exact kernel version
          echo "Runner kernel: $(uname -r)"
          LINUX_KERNEL=$(uname -r)

          # FIRST: Try pre-installed module (built for exact kernel version)
          MODULE_PATH=""
          echo "Looking for pre-installed kernel module..."
          for path in "/lib/modules/${LINUX_KERNEL}/extra/mlos-ml.ko" \
                      "/lib/modules/${LINUX_KERNEL}/extra/mlos_ml.ko" \
                      "/opt/mlos/kernel/mlos-ml.ko" \
                      "/opt/mlos/kernel/mlos_ml.ko"; do
            if [ -f "$path" ]; then
              MODULE_PATH="$path"
              echo "Found pre-installed module: $MODULE_PATH"
              break
            fi
          done

          # SECOND: If no pre-installed, try downloading from releases
          if [ -z "$MODULE_PATH" ] || [ ! -f "$MODULE_PATH" ]; then
            echo "No pre-installed module found, trying release download..."

            # Map to supported kernel versions
            KERNEL_MAJOR=$(echo "$LINUX_KERNEL" | cut -d. -f1,2)
            case "$KERNEL_MAJOR" in
              5.15*) KERNEL_SUFFIX="5.15" ;;
              6.1*)  KERNEL_SUFFIX="6.1" ;;
              6.5*)  KERNEL_SUFFIX="6.5" ;;
              6.8*)  KERNEL_SUFFIX="6.8" ;;
              *)
                echo "Unsupported kernel version: $KERNEL_MAJOR"
                KERNEL_SUFFIX="6.8"
                ;;
            esac

            MODULE_FILE="mlos-ml-v${KERNEL_MODULE_VERSION}-linux-${KERNEL_SUFFIX}.ko"
            DOWNLOAD_PATH="$HOME/${MODULE_FILE}"

            echo "Downloading kernel module: $MODULE_FILE"
            if gh release download "v${KERNEL_MODULE_VERSION}" \
                --repo mlOS-foundation/mlos-linux-kernel \
                --pattern "$MODULE_FILE" \
                --dir "$HOME" 2>/dev/null; then
              MODULE_PATH="$DOWNLOAD_PATH"
              echo "Downloaded module: $MODULE_PATH"
            else
              echo "WARNING: Could not download module from releases"
            fi
          fi

          if [ -z "$MODULE_PATH" ] || [ ! -f "$MODULE_PATH" ]; then
            echo "ERROR: No kernel module found!"
            echo "Please install the kernel module on the runner:"
            echo "  sudo cp mlos-ml.ko /lib/modules/$(uname -r)/extra/"
            echo "  sudo depmod -a"
            exit 1
          fi

          echo "module_path=$MODULE_PATH" >> $GITHUB_OUTPUT
          echo ""
          echo "Module info:"
          modinfo "$MODULE_PATH" 2>/dev/null || true

      - name: "Phase 2: Load kernel module"
        run: |
          MODULE_PATH="${{ steps.kernel-check.outputs.module_path }}"
          KERNEL_MODE="${{ github.event.inputs.kernel_mode || 'scheduler' }}"

          # Set parameters based on mode
          PARAMS="debug_level=2"
          case "$KERNEL_MODE" in
            basic)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=0 enable_gpu_manager=0"
              ;;
            scheduler)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=0"
              ;;
            full)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=1"
              ;;
          esac

          # Check module vermagic vs running kernel
          echo "Runner kernel: $(uname -r)"
          echo "Module vermagic: $(modinfo -F vermagic "$MODULE_PATH" 2>/dev/null || echo 'unknown')"

          echo "Loading module with params: $PARAMS"
          if ! sudo insmod "$MODULE_PATH" $PARAMS 2>&1; then
            echo ""
            echo "ERROR: Failed to load kernel module!"
            echo ""
            echo "This usually means the module was built for a different kernel version."
            echo "Module vermagic must match: $(uname -r)"
            echo ""
            echo "To fix: Build and install the module for kernel $(uname -r)"
            echo "  1. On a machine with matching kernel headers:"
            echo "     make -C /lib/modules/\$(uname -r)/build M=\$PWD modules"
            echo "  2. Copy to runner:"
            echo "     sudo cp mlos-ml.ko /lib/modules/$(uname -r)/extra/"
            echo "     sudo depmod -a"
            echo ""
            dmesg | tail -10
            exit 1
          fi

          sleep 2
          if lsmod | grep -q "mlos_ml\|mlos-ml"; then
            echo "Kernel module loaded successfully"
            lsmod | grep mlos
          else
            echo "ERROR: Module load command succeeded but module not in lsmod"
            dmesg | tail -30
            exit 1
          fi

      - name: "Phase 2: Start Core (Kernel Mode)"
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)

          BINARY_DIR=$(dirname "$BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"

          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"

          echo "Starting Core with KERNEL MODULE..."
          nohup $BINARY --http-port 8080 > /tmp/core-kernel.log 2>&1 &
          echo $! > /tmp/core-kernel.pid

          for i in {1..30}; do
            if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
              echo "Core started (kernel mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 2: Run E2E tests (Kernel)"
        id: test-kernel
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-kernel
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (KERNEL): $MODELS"
          TEST_START=$(date +%s%3N)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            echo "----------------------------------------"
            echo "Testing: $model (kernel mode)"
            echo "----------------------------------------"
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-kernel \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model had issues"
          done

          TEST_END=$(date +%s%3N)
          echo "kernel_duration_ms=$((TEST_END - TEST_START))" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 2: Stop Core (Kernel)"
        run: |
          if [ -f /tmp/core-kernel.pid ]; then
            kill $(cat /tmp/core-kernel.pid) 2>/dev/null || true
            rm /tmp/core-kernel.pid
          fi

      - name: "Phase 2: Unload kernel module"
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          echo "Kernel module unloaded"

      # ========================================================================
      # GENERATE UNIFIED REPORT
      # ========================================================================
      - name: Collect hardware and timing info
        if: always()
        run: |
          mkdir -p model-results

          # Collect hardware info (same as e2e-parallel)
          cat > model-results/hardware-info.json << EOF
          {
            "os": "$(uname -s)",
            "os_version": "$(cat /etc/os-release 2>/dev/null | grep PRETTY_NAME | cut -d'"' -f2 || uname -r)",
            "arch": "$(uname -m)",
            "cpu_model": "$(lscpu 2>/dev/null | grep 'Model name' | cut -d':' -f2 | xargs || echo 'Unknown')",
            "cpu_cores": $(nproc --all 2>/dev/null || echo 2),
            "cpu_threads": $(nproc 2>/dev/null || echo 2),
            "memory_gb": $(free -g 2>/dev/null | awk '/^Mem:/{print $2}' || echo 7),
            "gpu_count": $(nvidia-smi -L 2>/dev/null | wc -l || echo 0),
            "gpu_name": "$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -1 || echo 'None')",
            "gpu_memory": "$(nvidia-smi --query-gpu=memory.total --format=csv,noheader 2>/dev/null | head -1 || echo 'N/A')",
            "disk_total": "$(df -h / | tail -1 | awk '{print $2}')",
            "disk_available": "$(df -h / | tail -1 | awk '{print $4}')"
          }
          EOF

          # Collect timing info
          cat > model-results/timings-info.json << EOF
          {
            "axon_download_ms": 0,
            "core_download_ms": 0,
            "core_startup_ms": ${{ steps.core-userspace-start.outputs.core_startup_ms || 0 }}
          }
          EOF

          # Collect resource usage info from test steps
          cat > model-results/resources-info.json << EOF
          {
            "core_idle_cpu": ${{ steps.core-userspace-start.outputs.core_idle_cpu || 0 }},
            "core_idle_mem_mb": ${{ steps.core-userspace-start.outputs.core_idle_mem_mb || 0 }},
            "core_load_cpu_avg": ${{ steps.test-userspace.outputs.core_load_cpu_max || 0 }},
            "core_load_cpu_max": ${{ steps.test-userspace.outputs.core_load_cpu_max || 0 }},
            "core_load_mem_avg_mb": ${{ steps.test-userspace.outputs.core_load_mem_max_mb || 0 }},
            "core_load_mem_max_mb": ${{ steps.test-userspace.outputs.core_load_mem_max_mb || 0 }},
            "axon_cpu": 0,
            "axon_mem_mb": 0,
            "gpu_status": "Not used (CPU-only inference)",
            "kernel_mode": "${{ github.event.inputs.kernel_mode || 'scheduler' }}",
            "kernel_module_loaded": true
          }
          EOF

          # Copy userspace results to model-results (primary test results)
          cp model-results-userspace/*-result.json model-results/ 2>/dev/null || true

          echo "Hardware Info:"
          cat model-results/hardware-info.json

      - name: Aggregate userspace results
        if: always()
        env:
          AXON_VERSION: ${{ github.event.inputs.axon_version || 'v3.1.9' }}
          CORE_VERSION: ${{ github.event.inputs.core_version || '5.0.1-alpha' }}
        run: |
          mkdir -p output metrics

          python3 scripts/aggregate-results.py \
            --results-dir model-results \
            --output metrics/latest.json \
            --markdown output/RESULTS.md \
            --hardware-info model-results/hardware-info.json \
            --timings-info model-results/timings-info.json \
            --resources-info model-results/resources-info.json \
            --github-summary || echo "Aggregation completed"

      - name: Generate kernel comparison and merge
        if: always()
        env:
          KERNEL_MODE: ${{ github.event.inputs.kernel_mode || 'scheduler' }}
          KERNEL_MODULE_VERSION: ${{ steps.kernel-check.outputs.kernel_module_version || '5.0.1-alpha' }}
        run: |
          python3 << 'KERNEL_SCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime

          def get_inference_time(data):
              phases = data.get("phases", {})
              if "inference_large" in phases:
                  return phases["inference_large"].get("time_ms", 0)
              elif "inference_small" in phases:
                  return phases["inference_small"].get("time_ms", 0)
              return data.get("inference_time_ms", 0)

          # Load userspace results
          userspace_results = {}
          user_dir = Path("model-results-userspace")
          if user_dir.exists():
              for f in user_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      userspace_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Load kernel results
          kernel_results = {}
          kernel_dir = Path("model-results-kernel")
          if kernel_dir.exists():
              for f in kernel_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      kernel_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Calculate speedups
          speedup = {}
          for model in kernel_results:
              if model in userspace_results:
                  k_inf = kernel_results[model]["inference_ms"]
                  u_inf = userspace_results[model]["inference_ms"]
                  if k_inf > 0 and u_inf > 0:
                      speedup[model] = round(u_inf / k_inf, 2)

          avg_speedup = 0
          if speedup:
              valid = [v for v in speedup.values() if v > 0]
              if valid:
                  avg_speedup = round(sum(valid) / len(valid), 2)

          # Load hardware info
          import os
          hardware = {}
          hw_file = Path("model-results/hardware-info.json")
          if hw_file.exists():
              try:
                  with open(hw_file) as f:
                      hardware = json.load(f)
              except Exception as e:
                  print(f"Warning: Could not load hardware info: {e}")

          # Build kernel comparison
          kernel_comparison = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "kernel_mode": os.environ.get("KERNEL_MODE", "scheduler"),
              "kernel_module_version": os.environ.get("KERNEL_MODULE_VERSION", "5.0.1-alpha"),
              "models_tested": len(set(list(kernel_results.keys()) + list(userspace_results.keys()))),
              "comparison_enabled": True,
              "hardware": hardware,
              "kernel_results": kernel_results,
              "userspace_results": userspace_results,
              "speedup": speedup,
              "average_speedup": avg_speedup
          }

          # Merge into metrics
          metrics_file = Path("metrics/latest.json")
          if metrics_file.exists():
              with open(metrics_file) as f:
                  metrics = json.load(f)
              metrics["kernel_comparison"] = kernel_comparison
              with open(metrics_file, "w") as f:
                  json.dump(metrics, f, indent=2)
              print("Merged kernel comparison into metrics")

          # Also save standalone
          with open("metrics/kernel-comparison.json", "w") as f:
              json.dump(kernel_comparison, f, indent=2)

          # Print summary
          print(f"\nKernel Comparison Summary:")
          print(f"  Models tested: {len(speedup)}")
          print(f"  Kernel faster: {len([s for s in speedup.values() if s > 1.0])}")
          print(f"  Userspace faster: {len([s for s in speedup.values() if s < 1.0])}")
          print(f"  Average speedup: {avg_speedup}x")
          KERNEL_SCRIPT

      - name: Track historical metrics
        if: always() && github.event.inputs.track_history != 'false'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Download previous history from latest successful run (unless resetting)
          if [ "${{ github.event.inputs.reset_history }}" != "true" ]; then
            echo "Looking for previous historical data..."

            # Find latest successful run with metrics artifact
            PREV_RUN=$(gh run list --workflow="e2e-unified.yml" --status=success --limit=10 --json databaseId,conclusion \
              --jq '[.[] | select(.conclusion=="success")] | .[0].databaseId // empty')

            if [ -n "$PREV_RUN" ] && [ "$PREV_RUN" != "${{ github.run_id }}" ]; then
              echo "Downloading history from run $PREV_RUN..."

              # Get artifact name pattern
              ARTIFACT_NAME=$(gh api "repos/${{ github.repository }}/actions/runs/${PREV_RUN}/artifacts" \
                --jq '.artifacts[] | select(.name | startswith("unified-e2e-metrics")) | .name' | head -1)

              if [ -n "$ARTIFACT_NAME" ]; then
                mkdir -p prev_metrics
                gh run download "$PREV_RUN" -n "$ARTIFACT_NAME" -D prev_metrics/ 2>/dev/null || true

                if [ -f prev_metrics/history.json ]; then
                  echo "Found previous history with $(cat prev_metrics/history.json | python3 -c 'import sys,json; print(len(json.load(sys.stdin).get("runs",[])))') runs"
                  cp prev_metrics/history.json metrics/history.json
                fi
                rm -rf prev_metrics
              fi
            else
              echo "No previous run found or this is the first run"
            fi
          else
            echo "Resetting historical metrics..."
            python3 scripts/historical-metrics.py reset --history metrics/history.json
          fi

          # Add this run to history
          if [ -f metrics/latest.json ]; then
            echo "Adding run to historical metrics..."
            python3 scripts/historical-metrics.py add \
              --metrics metrics/latest.json \
              --history metrics/history.json

            # Calculate statistics
            echo "Calculating statistics..."
            python3 scripts/historical-metrics.py stats \
              --history metrics/history.json \
              --output metrics/statistics.json

            # Show summary
            python3 scripts/historical-metrics.py summary --history metrics/history.json
          fi

      # ========================================================================
      # UPLOAD ARTIFACTS (Report rendering is done by render-report.yml)
      # ========================================================================
      - name: Upload metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-metrics-${{ github.run_number }}
          path: metrics/
          retention-days: 90

      - name: Upload model results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-results-${{ github.run_number }}
          path: |
            model-results-userspace/
            model-results-kernel/
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## E2E Unified Test Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Metrics uploaded. To render the report, run the **Render E2E Report** workflow." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Quick Render" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "gh workflow run render-report.yml -f metrics_run_id=${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f metrics/latest.json ]; then
            echo "### Kernel Comparison Summary" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          with open('metrics/latest.json') as f:
              m = json.load(f)
          kc = m.get('kernel_comparison', {})
          print(f\"- Models tested: {kc.get('models_tested', 0)}\")
          print(f\"- Average speedup: {kc.get('average_speedup', 0)}x\")
          speedup = kc.get('speedup', {})
          print(f\"- Kernel faster: {len([s for s in speedup.values() if s > 1.0])}\")
          print(f\"- Userspace faster: {len([s for s in speedup.values() if s < 1.0])}\")
          " >> $GITHUB_STEP_SUMMARY
          fi

      # ========================================================================
      # CLEANUP
      # ========================================================================
      - name: Post-run cleanup
        if: always()
        run: |
          echo "========================================"
          echo "POST-RUN CLEANUP"
          echo "========================================"

          # Unload kernel module
          echo "Unloading kernel module..."
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # Stop Core processes
          echo "Stopping Core processes..."
          pkill -f mlos_core 2>/dev/null || true

          # Clean up caches to free disk space
          echo "Purging caches..."
          rm -rf ~/.cache/huggingface 2>/dev/null || true
          rm -rf ~/.cache/torch 2>/dev/null || true
          rm -rf ~/.axon/cache 2>/dev/null || true
          rm -rf ~/mlos-core 2>/dev/null || true
          rm -rf /tmp/axon* /tmp/model* /tmp/onnx* /tmp/hf* /tmp/torch* 2>/dev/null || true

          # Clean up downloaded kernel modules
          rm -rf ~/mlos-ml-*.ko 2>/dev/null || true

          # Sync filesystem
          sync

          echo "Disk space after cleanup:"
          df -h /
          echo ""
          echo "Cleanup complete"
