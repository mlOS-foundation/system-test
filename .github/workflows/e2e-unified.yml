name: E2E Unified Pipeline (Kernel + Userspace)

# Single workflow that runs BOTH userspace and kernel tests on the SAME machine
# for accurate, consistent performance comparison.

on:
  schedule:
    - cron: '0 6 * * 0'  # Weekly on Sunday at 6 AM UTC
  workflow_dispatch:
    inputs:
      axon_version:
        description: 'Axon version to test'
        required: false
        default: 'v3.1.9'
      core_version:
        description: 'Core version to test'
        required: false
        default: '5.0.1-alpha'
      kernel_mode:
        description: 'Kernel module mode'
        required: true
        type: choice
        options:
          - basic
          - scheduler
          - full
        default: 'scheduler'
      models:
        description: 'Models to test (space-separated, empty = all enabled)'
        required: false
        default: ''

permissions:
  contents: read
  actions: read
  packages: read

jobs:
  unified-e2e-test:
    name: E2E Test (Unified - Kernel + Userspace)
    runs-on: [self-hosted, linux, kernel-capable]

    steps:
      # ========================================================================
      # SETUP & CLEANUP
      # ========================================================================
      - name: Clean up from previous runs
        run: |
          echo "========================================"
          echo "Cleaning up stale state from previous runs"
          echo "========================================"

          # Stop any running Core processes
          pkill -f mlos_core 2>/dev/null || true
          sleep 2

          # Unload kernel module if loaded
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # AGGRESSIVE cache cleanup - remove ALL axon and huggingface caches
          echo "Removing model caches..."
          rm -rf ~/.axon 2>/dev/null || true
          rm -rf ~/.cache/huggingface 2>/dev/null || true
          rm -rf ~/mlos-core/models 2>/dev/null || true
          rm -rf /tmp/axon* /tmp/model* /tmp/onnx* 2>/dev/null || true

          # Clean up test directories
          rm -rf model-results-userspace model-results-kernel model-results metrics output 2>/dev/null || true

          echo "Disk space available:"
          df -h /

      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install pyyaml pillow numpy transformers torch torchvision jinja2 requests
          sudo apt-get update && sudo apt-get install -y curl tar jq

      - name: Download golden images
        run: |
          chmod +x scripts/download-golden-images.sh
          ./scripts/download-golden-images.sh

      - name: Get enabled models
        id: get-models
        run: |
          MODELS_INPUT=$(echo "${{ github.event.inputs.models }}" | tr ',' ' ' | xargs)

          if [ -n "$MODELS_INPUT" ]; then
            MODELS="$MODELS_INPUT"
          else
            MODELS=$(python3 -c "
          import yaml
          with open('config/models.yaml') as f:
              config = yaml.safe_load(f)
          models = [name for name, m in config.get('models', {}).items() if m.get('enabled', False)]
          print(' '.join(models))
          ")
          fi

          echo "models=$MODELS" >> $GITHUB_OUTPUT
          echo "Testing models: $MODELS"

      - name: Collect hardware info
        id: hardware
        run: |
          echo "os_name=$(lsb_release -ds 2>/dev/null || cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)" >> $GITHUB_OUTPUT
          echo "kernel_version=$(uname -r)" >> $GITHUB_OUTPUT
          echo "cpu_model=$(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> $GITHUB_OUTPUT
          echo "cpu_cores=$(nproc)" >> $GITHUB_OUTPUT
          echo "memory_gb=$(free -g | awk '/^Mem:/{print $2}')" >> $GITHUB_OUTPUT

          if command -v nvidia-smi &> /dev/null; then
            GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
          else
            GPU_NAME="None (CPU only)"
          fi
          echo "gpu_name=$GPU_NAME" >> $GITHUB_OUTPUT

          echo "Hardware Info:"
          echo "  OS: $(lsb_release -ds 2>/dev/null || echo 'Linux')"
          echo "  CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
          echo "  Cores: $(nproc)"
          echo "  Memory: $(free -g | awk '/^Mem:/{print $2}') GB"
          echo "  GPU: $GPU_NAME"

      # ========================================================================
      # DOWNLOAD RELEASES (using proven patterns from e2e-kernel.yml)
      # ========================================================================
      - name: Download Axon
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/.local/bin
          curl -L -f -o /tmp/axon.tar.gz \
            "https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/axon_${VERSION#v}_linux_${ARCH}.tar.gz"
          tar -xzf /tmp/axon.tar.gz -C ~/.local/bin
          chmod +x ~/.local/bin/axon
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          ~/.local/bin/axon version

      - name: Download converter image
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          VERSION_NO_V=${VERSION#v}
          OS="linux"
          ARCH=$(uname -m)
          case $ARCH in
            x86_64) ARCH="amd64" ;;
            aarch64) ARCH="arm64" ;;
          esac

          CONVERTER="axon-converter-${VERSION_NO_V}-${OS}-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/${CONVERTER}"
          IMAGE_TAG="ghcr.io/mlos-foundation/axon-converter:${VERSION_NO_V}"

          echo "Downloading converter image..."
          if curl -L -f -o "/tmp/${CONVERTER}" "$URL" 2>/dev/null; then
            docker load -i "/tmp/${CONVERTER}"
            docker tag "$IMAGE_TAG" "ghcr.io/mlos-foundation/axon-converter:latest" || true
            echo "Converter image loaded"
          else
            echo "WARNING: Converter image not available"
          fi

      - name: Download Core
        run: |
          VERSION="${{ github.event.inputs.core_version || '5.0.1-alpha' }}"
          VERSION_NO_V="${VERSION#v}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/mlos-core

          # Use public core-releases repo (core is private)
          ARTIFACT="mlos-core_${VERSION_NO_V}_linux-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/core-releases/releases/download/v${VERSION_NO_V}/${ARTIFACT}"

          echo "Downloading Core from: $URL"
          curl -L -f -o /tmp/$ARTIFACT "$URL"
          tar -xzf /tmp/$ARTIFACT -C ~/mlos-core

          # Download ONNX Runtime
          ONNX_ARCH="x64"; [ "$(uname -m)" = "aarch64" ] && ONNX_ARCH="aarch64"
          curl -L -f -o /tmp/onnx.tgz \
            "https://github.com/microsoft/onnxruntime/releases/download/v1.18.0/onnxruntime-linux-${ONNX_ARCH}-1.18.0.tgz"

          rm -rf ~/mlos-core/build/onnxruntime 2>/dev/null || true
          mkdir -p ~/mlos-core/build
          tar -xzf /tmp/onnx.tgz -C ~/mlos-core/build
          mv ~/mlos-core/build/onnxruntime-* ~/mlos-core/build/onnxruntime

          echo "Core downloaded and extracted"
          ls -la ~/mlos-core/

      # ========================================================================
      # PHASE 1: USERSPACE TESTS (No kernel module)
      # ========================================================================
      - name: "Phase 1: Ensure kernel module is NOT loaded"
        run: |
          echo "========================================"
          echo "PHASE 1: USERSPACE TESTS"
          echo "========================================"
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          if lsmod | grep -q "mlos"; then
            echo "ERROR: Kernel module still loaded!"
            exit 1
          fi
          echo "Kernel module confirmed NOT loaded"

      - name: "Phase 1: Start Core (Userspace Mode)"
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)
          chmod +x "$BINARY"

          BINARY_DIR=$(dirname "$BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"

          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"

          echo "Starting Core in USERSPACE mode..."
          nohup $BINARY --http-port 8080 > /tmp/core-userspace.log 2>&1 &
          echo $! > /tmp/core-userspace.pid

          for i in {1..30}; do
            if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
              echo "Core started (userspace mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 1: Run E2E tests (Userspace)"
        id: test-userspace
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-userspace
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (USERSPACE): $MODELS"
          TEST_START=$(date +%s%3N)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            echo "----------------------------------------"
            echo "Testing: $model (userspace)"
            echo "----------------------------------------"
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-userspace \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model had issues"
          done

          TEST_END=$(date +%s%3N)
          echo "userspace_duration_ms=$((TEST_END - TEST_START))" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 1: Stop Core (Userspace)"
        run: |
          if [ -f /tmp/core-userspace.pid ]; then
            kill $(cat /tmp/core-userspace.pid) 2>/dev/null || true
            rm /tmp/core-userspace.pid
          fi
          pkill -f mlos_core 2>/dev/null || true
          sleep 2
          echo "Core (userspace) stopped"

      - name: "Phase 1->2: Clean model cache for fresh kernel tests"
        run: |
          echo "Cleaning model cache between phases..."
          rm -rf ~/.axon/cache/models 2>/dev/null || true
          rm -rf ~/.axon/cache 2>/dev/null || true
          rm -rf ~/mlos-core/models 2>/dev/null || true
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true
          echo "Model cache cleaned"

      # ========================================================================
      # PHASE 2: KERNEL TESTS (With kernel module)
      # ========================================================================
      - name: "Phase 2: Verify kernel module capability"
        id: kernel-check
        run: |
          echo "========================================"
          echo "PHASE 2: KERNEL TESTS"
          echo "========================================"

          MODULE_PATH=""
          for path in "/lib/modules/$(uname -r)/extra/mlos-ml.ko" \
                      "/opt/mlos/kernel/mlos-ml.ko" \
                      "$HOME/mlos-ml.ko"; do
            if [ -f "$path" ]; then
              MODULE_PATH="$path"
              break
            fi
          done

          if [ -z "$MODULE_PATH" ]; then
            echo "ERROR: mlos-ml.ko not found"
            exit 1
          fi

          echo "module_path=$MODULE_PATH" >> $GITHUB_OUTPUT
          echo "Found kernel module: $MODULE_PATH"
          modinfo "$MODULE_PATH" 2>/dev/null || true

      - name: "Phase 2: Load kernel module"
        run: |
          MODULE_PATH="${{ steps.kernel-check.outputs.module_path }}"
          KERNEL_MODE="${{ github.event.inputs.kernel_mode || 'scheduler' }}"

          # Set parameters based on mode
          PARAMS="debug_level=2"
          case "$KERNEL_MODE" in
            basic)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=0 enable_gpu_manager=0"
              ;;
            scheduler)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=0"
              ;;
            full)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=1"
              ;;
          esac

          echo "Loading module with params: $PARAMS"
          sudo insmod "$MODULE_PATH" $PARAMS

          sleep 2
          if lsmod | grep -q "mlos_ml\|mlos-ml"; then
            echo "Kernel module loaded successfully"
            lsmod | grep mlos
          else
            echo "ERROR: Failed to load kernel module"
            dmesg | tail -30
            exit 1
          fi

      - name: "Phase 2: Start Core (Kernel Mode)"
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)

          BINARY_DIR=$(dirname "$BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"

          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"

          echo "Starting Core with KERNEL MODULE..."
          nohup $BINARY --http-port 8080 > /tmp/core-kernel.log 2>&1 &
          echo $! > /tmp/core-kernel.pid

          for i in {1..30}; do
            if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
              echo "Core started (kernel mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 2: Run E2E tests (Kernel)"
        id: test-kernel
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-kernel
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (KERNEL): $MODELS"
          TEST_START=$(date +%s%3N)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            echo "----------------------------------------"
            echo "Testing: $model (kernel mode)"
            echo "----------------------------------------"
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-kernel \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model had issues"
          done

          TEST_END=$(date +%s%3N)
          echo "kernel_duration_ms=$((TEST_END - TEST_START))" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 2: Stop Core (Kernel)"
        run: |
          if [ -f /tmp/core-kernel.pid ]; then
            kill $(cat /tmp/core-kernel.pid) 2>/dev/null || true
            rm /tmp/core-kernel.pid
          fi

      - name: "Phase 2: Unload kernel module"
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          echo "Kernel module unloaded"

      # ========================================================================
      # GENERATE UNIFIED REPORT
      # ========================================================================
      - name: Collect hardware and timing info
        if: always()
        run: |
          mkdir -p model-results

          # Collect hardware info (same as e2e-parallel)
          cat > model-results/hardware-info.json << EOF
          {
            "os": "$(uname -s)",
            "os_version": "$(cat /etc/os-release 2>/dev/null | grep PRETTY_NAME | cut -d'"' -f2 || uname -r)",
            "arch": "$(uname -m)",
            "cpu_model": "$(lscpu 2>/dev/null | grep 'Model name' | cut -d':' -f2 | xargs || echo 'Unknown')",
            "cpu_cores": $(nproc --all 2>/dev/null || echo 2),
            "cpu_threads": $(nproc 2>/dev/null || echo 2),
            "memory_gb": $(free -g 2>/dev/null | awk '/^Mem:/{print $2}' || echo 7),
            "gpu_count": $(nvidia-smi -L 2>/dev/null | wc -l || echo 0),
            "gpu_name": "$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -1 || echo 'None')",
            "gpu_memory": "$(nvidia-smi --query-gpu=memory.total --format=csv,noheader 2>/dev/null | head -1 || echo 'N/A')",
            "disk_total": "$(df -h / | tail -1 | awk '{print $2}')",
            "disk_available": "$(df -h / | tail -1 | awk '{print $4}')"
          }
          EOF

          # Collect timing info
          cat > model-results/timings-info.json << EOF
          {
            "axon_download_ms": 0,
            "core_download_ms": 0,
            "core_startup_ms": 0
          }
          EOF

          # Collect resource usage info
          cat > model-results/resources-info.json << EOF
          {
            "core_idle_cpu": 0,
            "core_idle_mem_mb": 0,
            "core_load_cpu_avg": 0,
            "core_load_cpu_max": 0,
            "core_load_mem_avg_mb": 0,
            "core_load_mem_max_mb": 0,
            "axon_cpu": 0,
            "axon_mem_mb": 0,
            "gpu_status": "Not used (CPU-only inference)",
            "kernel_mode": "${{ github.event.inputs.kernel_mode || 'scheduler' }}",
            "kernel_module_loaded": true
          }
          EOF

          # Copy userspace results to model-results (primary test results)
          cp model-results-userspace/*-result.json model-results/ 2>/dev/null || true

          echo "Hardware Info:"
          cat model-results/hardware-info.json

      - name: Aggregate userspace results
        if: always()
        env:
          AXON_VERSION: ${{ github.event.inputs.axon_version || 'v3.1.9' }}
          CORE_VERSION: ${{ github.event.inputs.core_version || '5.0.1-alpha' }}
        run: |
          mkdir -p output metrics

          python3 scripts/aggregate-results.py \
            --results-dir model-results \
            --output metrics/latest.json \
            --markdown output/RESULTS.md \
            --hardware-info model-results/hardware-info.json \
            --timings-info model-results/timings-info.json \
            --resources-info model-results/resources-info.json \
            --github-summary || echo "Aggregation completed"

      - name: Generate kernel comparison and merge
        if: always()
        env:
          KERNEL_MODE: ${{ github.event.inputs.kernel_mode || 'scheduler' }}
        run: |
          python3 << 'KERNEL_SCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime

          def get_inference_time(data):
              phases = data.get("phases", {})
              if "inference_large" in phases:
                  return phases["inference_large"].get("time_ms", 0)
              elif "inference_small" in phases:
                  return phases["inference_small"].get("time_ms", 0)
              return data.get("inference_time_ms", 0)

          # Load userspace results
          userspace_results = {}
          user_dir = Path("model-results-userspace")
          if user_dir.exists():
              for f in user_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      userspace_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Load kernel results
          kernel_results = {}
          kernel_dir = Path("model-results-kernel")
          if kernel_dir.exists():
              for f in kernel_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      kernel_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Calculate speedups
          speedup = {}
          for model in kernel_results:
              if model in userspace_results:
                  k_inf = kernel_results[model]["inference_ms"]
                  u_inf = userspace_results[model]["inference_ms"]
                  if k_inf > 0 and u_inf > 0:
                      speedup[model] = round(u_inf / k_inf, 2)

          avg_speedup = 0
          if speedup:
              valid = [v for v in speedup.values() if v > 0]
              if valid:
                  avg_speedup = round(sum(valid) / len(valid), 2)

          # Load hardware info
          import os
          hardware = {}
          hw_file = Path("model-results/hardware-info.json")
          if hw_file.exists():
              try:
                  with open(hw_file) as f:
                      hardware = json.load(f)
              except Exception as e:
                  print(f"Warning: Could not load hardware info: {e}")

          # Build kernel comparison
          kernel_comparison = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "kernel_mode": os.environ.get("KERNEL_MODE", "scheduler"),
              "kernel_module_version": "1.0.0",
              "models_tested": len(set(list(kernel_results.keys()) + list(userspace_results.keys()))),
              "comparison_enabled": True,
              "hardware": hardware,
              "kernel_results": kernel_results,
              "userspace_results": userspace_results,
              "speedup": speedup,
              "average_speedup": avg_speedup
          }

          # Merge into metrics
          metrics_file = Path("metrics/latest.json")
          if metrics_file.exists():
              with open(metrics_file) as f:
                  metrics = json.load(f)
              metrics["kernel_comparison"] = kernel_comparison
              with open(metrics_file, "w") as f:
                  json.dump(metrics, f, indent=2)
              print("Merged kernel comparison into metrics")

          # Also save standalone
          with open("metrics/kernel-comparison.json", "w") as f:
              json.dump(kernel_comparison, f, indent=2)

          # Print summary
          print(f"\nKernel Comparison Summary:")
          print(f"  Models tested: {len(speedup)}")
          print(f"  Kernel faster: {len([s for s in speedup.values() if s > 1.0])}")
          print(f"  Userspace faster: {len([s for s in speedup.values() if s < 1.0])}")
          print(f"  Average speedup: {avg_speedup}x")
          KERNEL_SCRIPT

      # ========================================================================
      # UPLOAD ARTIFACTS (Report rendering is done by render-report.yml)
      # ========================================================================
      - name: Upload metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-metrics-${{ github.run_number }}
          path: metrics/
          retention-days: 90

      - name: Upload model results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-results-${{ github.run_number }}
          path: |
            model-results-userspace/
            model-results-kernel/
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## E2E Unified Test Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Metrics uploaded. To render the report, run the **Render E2E Report** workflow." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Quick Render" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "gh workflow run render-report.yml -f metrics_run_id=${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f metrics/latest.json ]; then
            echo "### Kernel Comparison Summary" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          with open('metrics/latest.json') as f:
              m = json.load(f)
          kc = m.get('kernel_comparison', {})
          print(f\"- Models tested: {kc.get('models_tested', 0)}\")
          print(f\"- Average speedup: {kc.get('average_speedup', 0)}x\")
          speedup = kc.get('speedup', {})
          print(f\"- Kernel faster: {len([s for s in speedup.values() if s > 1.0])}\")
          print(f\"- Userspace faster: {len([s for s in speedup.values() if s < 1.0])}\")
          " >> $GITHUB_STEP_SUMMARY
          fi

      # ========================================================================
      # CLEANUP
      # ========================================================================
      - name: Post-run cleanup
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          pkill -f mlos_core 2>/dev/null || true
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true
          echo "Cleanup complete"
