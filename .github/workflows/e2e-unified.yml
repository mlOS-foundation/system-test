name: E2E Unified Pipeline (Kernel + Userspace)

# Single workflow that runs BOTH userspace and kernel tests on the SAME machine
# for accurate, consistent performance comparison.

on:
  schedule:
    - cron: '0 6 * * 0'  # Weekly on Sunday at 6 AM UTC
  workflow_dispatch:
    inputs:
      axon_version:
        description: 'Axon version to test'
        required: false
        default: 'v3.1.9'
      core_version:
        description: 'Core version to test'
        required: false
        default: '5.0.1-alpha'
      kernel_mode:
        description: 'Kernel module mode'
        required: true
        type: choice
        options:
          - basic
          - scheduler
          - full
        default: 'scheduler'
      models:
        description: 'Models to test (space-separated, empty = all enabled)'
        required: false
        default: ''

permissions:
  contents: write
  pages: write
  id-token: write
  packages: read

jobs:
  unified-e2e-test:
    name: E2E Test (Unified - Kernel + Userspace)
    runs-on: [self-hosted, linux, kernel-capable]
    environment:
      name: github-pages

    steps:
      # ========================================================================
      # SETUP & CLEANUP
      # ========================================================================
      - name: Clean up from previous runs
        run: |
          echo "========================================"
          echo "Cleaning up stale state from previous runs"
          echo "========================================"

          # Stop any running Core processes
          pkill -f mlos_core 2>/dev/null || true
          sleep 2

          # Unload kernel module if loaded
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          # Clean up caches
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true
          rm -rf ~/.axon/cache/models 2>/dev/null || true
          rm -rf ~/.axon/cache 2>/dev/null || true
          rm -rf ~/mlos-core/models 2>/dev/null || true

          # Clean up test directories
          rm -rf model-results-userspace model-results-kernel metrics output 2>/dev/null || true

          echo "Disk space available:"
          df -h /

      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install pyyaml pillow numpy transformers torch torchvision jinja2 requests
          sudo apt-get update && sudo apt-get install -y curl tar jq

      - name: Download golden images
        run: |
          chmod +x scripts/download-golden-images.sh
          ./scripts/download-golden-images.sh

      - name: Get enabled models
        id: get-models
        run: |
          MODELS_INPUT=$(echo "${{ github.event.inputs.models }}" | tr ',' ' ' | xargs)

          if [ -n "$MODELS_INPUT" ]; then
            MODELS="$MODELS_INPUT"
          else
            MODELS=$(python3 -c "
          import yaml
          with open('config/models.yaml') as f:
              config = yaml.safe_load(f)
          models = [name for name, m in config.get('models', {}).items() if m.get('enabled', False)]
          print(' '.join(models))
          ")
          fi

          echo "models=$MODELS" >> $GITHUB_OUTPUT
          echo "Testing models: $MODELS"

      - name: Collect hardware info
        id: hardware
        run: |
          echo "os_name=$(lsb_release -ds 2>/dev/null || cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)" >> $GITHUB_OUTPUT
          echo "kernel_version=$(uname -r)" >> $GITHUB_OUTPUT
          echo "cpu_model=$(lscpu | grep 'Model name' | cut -d: -f2 | xargs)" >> $GITHUB_OUTPUT
          echo "cpu_cores=$(nproc)" >> $GITHUB_OUTPUT
          echo "memory_gb=$(free -g | awk '/^Mem:/{print $2}')" >> $GITHUB_OUTPUT

          if command -v nvidia-smi &> /dev/null; then
            GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
          else
            GPU_NAME="None (CPU only)"
          fi
          echo "gpu_name=$GPU_NAME" >> $GITHUB_OUTPUT

          echo "Hardware Info:"
          echo "  OS: $(lsb_release -ds 2>/dev/null || echo 'Linux')"
          echo "  CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
          echo "  Cores: $(nproc)"
          echo "  Memory: $(free -g | awk '/^Mem:/{print $2}') GB"
          echo "  GPU: $GPU_NAME"

      # ========================================================================
      # DOWNLOAD RELEASES (using proven patterns from e2e-kernel.yml)
      # ========================================================================
      - name: Download Axon
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/.local/bin
          curl -L -f -o /tmp/axon.tar.gz \
            "https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/axon_${VERSION#v}_linux_${ARCH}.tar.gz"
          tar -xzf /tmp/axon.tar.gz -C ~/.local/bin
          chmod +x ~/.local/bin/axon
          echo "$HOME/.local/bin" >> $GITHUB_PATH

          ~/.local/bin/axon version

      - name: Download converter image
        run: |
          VERSION="${{ github.event.inputs.axon_version || 'v3.1.9' }}"
          VERSION_NO_V=${VERSION#v}
          OS="linux"
          ARCH=$(uname -m)
          case $ARCH in
            x86_64) ARCH="amd64" ;;
            aarch64) ARCH="arm64" ;;
          esac

          CONVERTER="axon-converter-${VERSION_NO_V}-${OS}-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/axon/releases/download/${VERSION}/${CONVERTER}"
          IMAGE_TAG="ghcr.io/mlos-foundation/axon-converter:${VERSION_NO_V}"

          echo "Downloading converter image..."
          if curl -L -f -o "/tmp/${CONVERTER}" "$URL" 2>/dev/null; then
            docker load -i "/tmp/${CONVERTER}"
            docker tag "$IMAGE_TAG" "ghcr.io/mlos-foundation/axon-converter:latest" || true
            echo "Converter image loaded"
          else
            echo "WARNING: Converter image not available"
          fi

      - name: Download Core
        run: |
          VERSION="${{ github.event.inputs.core_version || '5.0.1-alpha' }}"
          VERSION_NO_V="${VERSION#v}"
          ARCH=$(uname -m); [ "$ARCH" = "x86_64" ] && ARCH="amd64"

          mkdir -p ~/mlos-core

          # Use public core-releases repo (core is private)
          ARTIFACT="mlos-core_${VERSION_NO_V}_linux-${ARCH}.tar.gz"
          URL="https://github.com/mlOS-foundation/core-releases/releases/download/v${VERSION_NO_V}/${ARTIFACT}"

          echo "Downloading Core from: $URL"
          curl -L -f -o /tmp/$ARTIFACT "$URL"
          tar -xzf /tmp/$ARTIFACT -C ~/mlos-core

          # Download ONNX Runtime
          ONNX_ARCH="x64"; [ "$(uname -m)" = "aarch64" ] && ONNX_ARCH="aarch64"
          curl -L -f -o /tmp/onnx.tgz \
            "https://github.com/microsoft/onnxruntime/releases/download/v1.18.0/onnxruntime-linux-${ONNX_ARCH}-1.18.0.tgz"

          rm -rf ~/mlos-core/build/onnxruntime 2>/dev/null || true
          mkdir -p ~/mlos-core/build
          tar -xzf /tmp/onnx.tgz -C ~/mlos-core/build
          mv ~/mlos-core/build/onnxruntime-* ~/mlos-core/build/onnxruntime

          echo "Core downloaded and extracted"
          ls -la ~/mlos-core/

      # ========================================================================
      # PHASE 1: USERSPACE TESTS (No kernel module)
      # ========================================================================
      - name: "Phase 1: Ensure kernel module is NOT loaded"
        run: |
          echo "========================================"
          echo "PHASE 1: USERSPACE TESTS"
          echo "========================================"
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true

          if lsmod | grep -q "mlos"; then
            echo "ERROR: Kernel module still loaded!"
            exit 1
          fi
          echo "Kernel module confirmed NOT loaded"

      - name: "Phase 1: Start Core (Userspace Mode)"
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)
          chmod +x "$BINARY"

          BINARY_DIR=$(dirname "$BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"

          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"

          echo "Starting Core in USERSPACE mode..."
          nohup $BINARY --http-port 8080 > /tmp/core-userspace.log 2>&1 &
          echo $! > /tmp/core-userspace.pid

          for i in {1..30}; do
            if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
              echo "Core started (userspace mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 1: Run E2E tests (Userspace)"
        id: test-userspace
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-userspace
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (USERSPACE): $MODELS"
          TEST_START=$(date +%s%3N)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            echo "----------------------------------------"
            echo "Testing: $model (userspace)"
            echo "----------------------------------------"
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-userspace \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model had issues"
          done

          TEST_END=$(date +%s%3N)
          echo "userspace_duration_ms=$((TEST_END - TEST_START))" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 1: Stop Core (Userspace)"
        run: |
          if [ -f /tmp/core-userspace.pid ]; then
            kill $(cat /tmp/core-userspace.pid) 2>/dev/null || true
            rm /tmp/core-userspace.pid
          fi
          sleep 2
          echo "Core (userspace) stopped"

      # ========================================================================
      # PHASE 2: KERNEL TESTS (With kernel module)
      # ========================================================================
      - name: "Phase 2: Verify kernel module capability"
        id: kernel-check
        run: |
          echo "========================================"
          echo "PHASE 2: KERNEL TESTS"
          echo "========================================"

          MODULE_PATH=""
          for path in "/lib/modules/$(uname -r)/extra/mlos-ml.ko" \
                      "/opt/mlos/kernel/mlos-ml.ko" \
                      "$HOME/mlos-ml.ko"; do
            if [ -f "$path" ]; then
              MODULE_PATH="$path"
              break
            fi
          done

          if [ -z "$MODULE_PATH" ]; then
            echo "ERROR: mlos-ml.ko not found"
            exit 1
          fi

          echo "module_path=$MODULE_PATH" >> $GITHUB_OUTPUT
          echo "Found kernel module: $MODULE_PATH"
          modinfo "$MODULE_PATH" 2>/dev/null || true

      - name: "Phase 2: Load kernel module"
        run: |
          MODULE_PATH="${{ steps.kernel-check.outputs.module_path }}"
          KERNEL_MODE="${{ github.event.inputs.kernel_mode || 'scheduler' }}"

          # Set parameters based on mode
          PARAMS="debug_level=2"
          case "$KERNEL_MODE" in
            basic)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=0 enable_gpu_manager=0"
              ;;
            scheduler)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=0"
              ;;
            full)
              PARAMS="$PARAMS enable_tmm=1 enable_scheduler=1 enable_gpu_manager=1"
              ;;
          esac

          echo "Loading module with params: $PARAMS"
          sudo insmod "$MODULE_PATH" $PARAMS

          sleep 2
          if lsmod | grep -q "mlos_ml\|mlos-ml"; then
            echo "Kernel module loaded successfully"
            lsmod | grep mlos
          else
            echo "ERROR: Failed to load kernel module"
            dmesg | tail -30
            exit 1
          fi

      - name: "Phase 2: Start Core (Kernel Mode)"
        run: |
          cd ~/mlos-core
          BINARY=$(find . -name "mlos_core" -type f -executable | head -1)

          BINARY_DIR=$(dirname "$BINARY")
          LLAMA_LIB=""
          [ -d "$BINARY_DIR/llama.cpp/lib" ] && LLAMA_LIB="$BINARY_DIR/llama.cpp/lib"

          export LD_LIBRARY_PATH="$(pwd)/build/onnxruntime/lib:${LLAMA_LIB}:$LD_LIBRARY_PATH"

          echo "Starting Core with KERNEL MODULE..."
          nohup $BINARY --http-port 8080 > /tmp/core-kernel.log 2>&1 &
          echo $! > /tmp/core-kernel.pid

          for i in {1..30}; do
            if curl -s http://127.0.0.1:8080/health >/dev/null 2>&1; then
              echo "Core started (kernel mode)"
              break
            fi
            sleep 1
          done

      - name: "Phase 2: Run E2E tests (Kernel)"
        id: test-kernel
        run: |
          MODELS="${{ steps.get-models.outputs.models }}"
          mkdir -p model-results-kernel
          chmod +x scripts/test-single-model.sh
          export PATH="$HOME/.local/bin:$PATH"

          echo "Testing models (KERNEL): $MODELS"
          TEST_START=$(date +%s%3N)

          for model in $MODELS; do
            [ -z "$model" ] && continue
            echo "----------------------------------------"
            echo "Testing: $model (kernel mode)"
            echo "----------------------------------------"
            ./scripts/test-single-model.sh "$model" \
              --output-dir model-results-kernel \
              --core-url http://127.0.0.1:8080 \
              --golden-images \
              || echo "Warning: $model had issues"
          done

          TEST_END=$(date +%s%3N)
          echo "kernel_duration_ms=$((TEST_END - TEST_START))" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: "Phase 2: Stop Core (Kernel)"
        run: |
          if [ -f /tmp/core-kernel.pid ]; then
            kill $(cat /tmp/core-kernel.pid) 2>/dev/null || true
            rm /tmp/core-kernel.pid
          fi

      - name: "Phase 2: Unload kernel module"
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          echo "Kernel module unloaded"

      # ========================================================================
      # GENERATE UNIFIED REPORT
      # ========================================================================
      - name: Generate unified comparison report
        if: always()
        env:
          OS_NAME: ${{ steps.hardware.outputs.os_name }}
          KERNEL_VERSION: ${{ steps.hardware.outputs.kernel_version }}
          CPU_MODEL: ${{ steps.hardware.outputs.cpu_model }}
          CPU_CORES: ${{ steps.hardware.outputs.cpu_cores }}
          MEMORY_GB: ${{ steps.hardware.outputs.memory_gb }}
          GPU_NAME: ${{ steps.hardware.outputs.gpu_name }}
          KERNEL_MODE: ${{ github.event.inputs.kernel_mode || 'scheduler' }}
          AXON_VERSION: ${{ github.event.inputs.axon_version || 'v3.1.9' }}
          CORE_VERSION: ${{ github.event.inputs.core_version || '5.0.1-alpha' }}
        run: |
          python3 << 'REPORT_SCRIPT'
          import json
          import os
          from pathlib import Path
          from datetime import datetime

          def get_inference_time(data):
              """Extract inference time from result JSON."""
              phases = data.get("phases", {})
              if "inference_large" in phases:
                  return phases["inference_large"].get("time_ms", 0)
              elif "inference_small" in phases:
                  return phases["inference_small"].get("time_ms", 0)
              return data.get("inference_time_ms", 0)

          def get_install_time(data):
              """Extract install time from result JSON."""
              phases = data.get("phases", {})
              if "install" in phases:
                  return phases["install"].get("time_ms", 0)
              return data.get("install_time_ms", 0)

          # Hardware info
          hardware = {
              "os": os.environ.get("OS_NAME", "Linux"),
              "kernel_version": os.environ.get("KERNEL_VERSION", "unknown"),
              "cpu_model": os.environ.get("CPU_MODEL", "unknown"),
              "cpu_cores": int(os.environ.get("CPU_CORES", 0) or 0),
              "memory_gb": int(os.environ.get("MEMORY_GB", 0) or 0),
              "gpu_name": os.environ.get("GPU_NAME", "None"),
          }

          # Collect userspace results
          userspace_results = {}
          user_dir = Path("model-results-userspace")
          if user_dir.exists():
              for f in user_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      userspace_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                          "install_ms": get_install_time(data)
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Collect kernel results
          kernel_results = {}
          kernel_dir = Path("model-results-kernel")
          if kernel_dir.exists():
              for f in kernel_dir.glob("*-result.json"):
                  try:
                      with open(f) as fp:
                          data = json.load(fp)
                      model = f.stem.replace("-result", "")
                      kernel_results[model] = {
                          "status": data.get("status", "unknown"),
                          "inference_ms": get_inference_time(data),
                          "install_ms": get_install_time(data)
                      }
                  except Exception as e:
                      print(f"Error reading {f}: {e}")

          # Calculate speedups
          speedup = {}
          for model in kernel_results:
              if model in userspace_results:
                  k_inf = kernel_results[model]["inference_ms"]
                  u_inf = userspace_results[model]["inference_ms"]
                  if k_inf > 0 and u_inf > 0:
                      speedup[model] = round(u_inf / k_inf, 2)

          # Calculate average speedup
          avg_speedup = 0
          if speedup:
              valid_speedups = [v for v in speedup.values() if v > 0]
              if valid_speedups:
                  avg_speedup = round(sum(valid_speedups) / len(valid_speedups), 2)

          # Build metrics
          metrics = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "workflow": "unified",
              "axon_version": os.environ.get("AXON_VERSION", "unknown"),
              "core_version": os.environ.get("CORE_VERSION", "unknown"),
              "hardware": hardware,
              "kernel_comparison": {
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "kernel_mode": os.environ.get("KERNEL_MODE", "scheduler"),
                  "kernel_module_version": "1.0.0",
                  "models_tested": len(set(list(kernel_results.keys()) + list(userspace_results.keys()))),
                  "comparison_enabled": True,
                  "hardware": hardware,
                  "kernel_results": kernel_results,
                  "userspace_results": userspace_results,
                  "speedup": speedup,
                  "average_speedup": avg_speedup
              },
              "summary": {
                  "total_models": len(set(list(kernel_results.keys()) + list(userspace_results.keys()))),
                  "kernel_faster_count": len([s for s in speedup.values() if s > 1.0]),
                  "userspace_faster_count": len([s for s in speedup.values() if s < 1.0]),
                  "average_speedup": avg_speedup
              }
          }

          # Write metrics
          Path("metrics").mkdir(exist_ok=True)
          with open("metrics/latest.json", "w") as f:
              json.dump(metrics, f, indent=2)

          with open("metrics/kernel-comparison.json", "w") as f:
              json.dump(metrics["kernel_comparison"], f, indent=2)

          # Print summary
          print("\n" + "=" * 60)
          print("UNIFIED E2E TEST RESULTS")
          print("=" * 60)
          print(f"Hardware: {hardware['cpu_model']}")
          print(f"Models tested: {metrics['summary']['total_models']}")
          print(f"Kernel faster: {metrics['summary']['kernel_faster_count']}")
          print(f"Userspace faster: {metrics['summary']['userspace_faster_count']}")
          print(f"Average speedup: {avg_speedup}x")
          print()
          for model in sorted(speedup.keys()):
              k = kernel_results.get(model, {}).get("inference_ms", 0)
              u = userspace_results.get(model, {}).get("inference_ms", 0)
              s = speedup[model]
              winner = "KERNEL" if s > 1 else "USERSPACE" if s < 1 else "SAME"
              print(f"  {model}: K={k}ms U={u}ms -> {s}x ({winner})")
          REPORT_SCRIPT

      - name: Generate HTML report
        if: always()
        run: |
          python3 report/render.py \
            --metrics metrics/latest.json \
            --template report/template.html \
            --output output/index.html

      # ========================================================================
      # UPLOAD & DEPLOY
      # ========================================================================
      - name: Upload metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-metrics-${{ github.run_number }}
          path: metrics/
          retention-days: 90

      - name: Upload model results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unified-e2e-results-${{ github.run_number }}
          path: |
            model-results-userspace/
            model-results-kernel/
          retention-days: 30

      - name: Setup Pages
        if: always()
        uses: actions/configure-pages@v4

      - name: Prepare Pages artifact
        if: always()
        run: |
          mkdir -p _site
          cp output/index.html _site/ 2>/dev/null || echo "No HTML report"
          cp -r metrics _site/ 2>/dev/null || true

      - name: Upload Pages artifact
        if: always()
        uses: actions/upload-pages-artifact@v3
        with:
          path: _site

      - name: Deploy to GitHub Pages
        if: always()
        uses: actions/deploy-pages@v4

      # ========================================================================
      # CLEANUP
      # ========================================================================
      - name: Post-run cleanup
        if: always()
        run: |
          sudo rmmod mlos_ml 2>/dev/null || sudo rmmod mlos-ml 2>/dev/null || true
          pkill -f mlos_core 2>/dev/null || true
          rm -rf ~/.cache/huggingface/hub 2>/dev/null || true
          echo "Cleanup complete"
