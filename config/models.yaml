# MLOS E2E Test - Model Configuration
# Add new models here to include them in test runs
# 
# Updated: 2024-12-01 - Added validated vision models (Axon v3.1.0, Core v3.2.0-alpha)

# Test runner settings
settings:
  # Maximum time (seconds) to wait for model installation
  install_timeout: 600
  # Maximum time (seconds) to wait for inference
  inference_timeout: 120  # Increased for vision models (larger inputs)
  # Run large inference tests (more tokens/larger inputs)
  run_large_tests: true

# Model definitions
# Each model needs:
#   - enabled: whether to test this model
#   - category: nlp | vision | multimodal
#   - axon_id: the Axon model identifier (hf/namespace/model@version)
#   - input_type: text | image | audio
#   - small_input: input config for small/quick test
#   - large_input: input config for large/stress test

models:
  # =============================================================================
  # NLP Models (Text-based)
  # =============================================================================
  
  gpt2:
    enabled: true
    category: nlp
    axon_id: "hf/distilgpt2@latest"
    description: "DistilGPT-2 - Lightweight text generation"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]  # "What is the capital"
    large_input:
      tokens: 128
      # Generated dynamically

  bert:
    enabled: true
    category: nlp
    axon_id: "hf/bert-base-uncased@latest"
    description: "BERT base - Masked language model"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  roberta:
    enabled: true
    category: nlp
    axon_id: "hf/roberta-base@latest"
    description: "RoBERTa base - Robust BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  t5:
    enabled: false  # Disabled - encoder-decoder architecture needs special handling
    category: nlp
    axon_id: "hf/t5-small@latest"
    description: "T5 small - Text-to-text transformer"
    input_type: text
    notes: "Requires encoder-decoder input format"
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Vision Models (Image-based) - Validated with Axon v3.1.0 + Core v3.2.0-alpha
  # =============================================================================
  
  resnet:
    enabled: true
    category: vision
    axon_id: "hf/microsoft/resnet-50@latest"
    description: "ResNet-50 - Image classification (1000 classes)"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  vit:
    enabled: true
    category: vision
    axon_id: "hf/google/vit-base-patch16-224@latest"
    description: "Vision Transformer (ViT) - Image classification"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  convnext:
    enabled: true
    category: vision
    axon_id: "hf/facebook/convnext-tiny-224@latest"
    description: "ConvNeXt Tiny - Modern CNN architecture"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  mobilenet:
    enabled: true
    category: vision
    axon_id: "hf/google/mobilenet_v2_1.0_224@latest"
    description: "MobileNetV2 - Efficient mobile architecture"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  deit:
    enabled: true
    category: vision
    axon_id: "hf/facebook/deit-small-patch16-224@latest"
    description: "DeiT Small - Data-efficient Image Transformer"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  # Additional vision models (not yet validated)
  swin:
    enabled: false  # Known issue: PyTorch-to-ONNX export fails with dynamic shapes
    category: vision
    axon_id: "hf/microsoft/swin-tiny-patch4-window7-224@latest"
    description: "Swin Transformer - Shifted window attention"
    input_type: image
    notes: "Known PyTorch-to-ONNX conversion issues"
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  efficientnet:
    enabled: true  # Validated and enabled
    category: vision
    axon_id: "hf/google/efficientnet-b0@latest"
    description: "EfficientNet-B0 - Compound scaling CNN"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  # =============================================================================
  # Multimodal Models
  # =============================================================================
  
  clip:
    enabled: false  # Blocked: Multi-encoder ONNX export (text_model.onnx + vision_model.onnx)
    category: multimodal
    axon_id: "hf/openai/clip-vit-base-patch32@latest"
    description: "CLIP - Image-text matching and zero-shot classification"
    input_type: multimodal
    notes: "Optimum exports separate files (text_model.onnx, vision_model.onnx) - need Axon/Core multi-encoder support"
    small_input:
      text_tokens: 77
      image_width: 224
      image_height: 224
    large_input:
      text_tokens: 77
      image_width: 224
      image_height: 224

  wav2vec2:
    enabled: false  # Blocked: Audio model support in Core
    category: multimodal
    axon_id: "hf/facebook/wav2vec2-base-960h@latest"
    description: "Wav2Vec2 - Speech recognition"
    input_type: audio
    notes: "Audio models require waveform input - pending Core support"
    small_input:
      duration_seconds: 1.0
      sample_rate: 16000
    large_input:
      duration_seconds: 5.0
      sample_rate: 16000

# =============================================================================
# Multi-Encoder / Encoder-Decoder Support Roadmap
# =============================================================================
# The following models require multi-ONNX-file support in Axon and Core:
#
# 1. CLIP (Multi-Encoder):
#    - Exports: text_model.onnx + vision_model.onnx
#    - Requires: Combined inference orchestration
#
# 2. T5/BART (Encoder-Decoder):
#    - Exports: encoder_model.onnx + decoder_model.onnx + decoder_with_past_model.onnx
#    - Requires: Cross-attention between encoder/decoder
#
# Tracking: https://github.com/mlOS-foundation/axon/issues/TBD
#

# =============================================================================
# Quick reference for adding new models:
# =============================================================================
#
# 1. Add entry under 'models:' section
# 2. Set 'enabled: true' to include in tests
# 3. Choose correct category: nlp, vision, or multimodal
# 4. Provide the Axon model ID (run 'axon search <model>' to find it)
# 5. Configure input sizes appropriate for the model
#
# Vision model notes:
# - Most vision models expect 224x224 inputs (standard ImageNet size)
# - Use smaller sizes (64x64) for quick smoke tests
# - ONNX conversion via Axon v3.1.0+ handles task detection automatically
# - Core v3.2.0-alpha+ supports large image payloads (up to 16MB)
#
# Example:
#   my_new_model:
#     enabled: true
#     category: nlp
#     axon_id: "hf/my-org/my-model@latest"
#     description: "My awesome model"
#     input_type: text
#     small_input:
#       tokens: 10
#     large_input:
#       tokens: 128

