# MLOS E2E Test - Model Configuration
# Add new models here to include them in test runs
#
# Updated: 2024-12-11 - Extended LLM support (Core v4.0.0-alpha, Axon v3.1.7)
# - Seq2seq + Multi-encoder support (Axon v3.1.6+, Core v3.2.9-alpha+)
# - T5 enabled with encoder-decoder architecture support (fixed in v3.2.14-alpha, v4.0.0-alpha)
# - CLIP enabled with multi-encoder architecture support
# - LLM/GGUF support with top models: Qwen2, Llama 3.2, DeepSeek, TinyLlama, Phi-2
# - execution_files: Explicit model paths in manifest (no directory guessing)
#
# Format-agnostic runtime plugin system (Core v3.2.13-alpha):
#   - ONNX (.onnx) - Built-in via ONNX Runtime
#   - GGUF (.gguf) - llama.cpp LLM plugin (native execution)
#   - PyTorch (.pt, .pth, .bin, .safetensors) - Auto-converted to ONNX
#   - TFLite (.tflite) - Plugin ready
#   - TensorFlow (.pb) - Plugin ready
#   - CoreML (.mlmodel, .mlpackage) - Plugin ready
#
# Release versions:
#   - Core v4.0.0-alpha: Kernel module + T5 seq2seq + GGUF/LLM runtime
#   - Axon v3.1.7: execution_files in manifest for explicit model paths

# Test runner settings
settings:
  # Maximum time (seconds) to wait for model installation
  install_timeout: 900  # 15 minutes (increased for larger models)
  # Maximum time (seconds) for LLM/GGUF model installation (large downloads)
  llm_install_timeout: 2400  # 40 minutes for large GGUF files (500MB+)
  # Maximum time (seconds) to wait for inference
  inference_timeout: 120  # Increased for vision models (larger inputs)
  # Maximum time (seconds) for LLM generation inference
  llm_inference_timeout: 300  # 5 minutes for LLM text generation
  # Run large inference tests (more tokens/larger inputs)
  run_large_tests: true
  # Enable LLM generation tests (GGUF plugin ready in Core v3.2.10-alpha)
  enable_llm_tests: true

# Model definitions
# Each model needs:
#   - enabled: whether to test this model
#   - category: nlp | vision | multimodal
#   - axon_id: the Axon model identifier (hf/namespace/model@version)
#   - input_type: text | image | audio
#   - small_input: input config for small/quick test
#   - large_input: input config for large/stress test

models:
  # =============================================================================
  # NLP Models (Text-based)
  # =============================================================================
  
  gpt2:
    enabled: true
    category: nlp
    axon_id: "hf/distilgpt2@latest"
    description: "DistilGPT-2 - Lightweight text generation"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]  # "What is the capital"
    large_input:
      tokens: 128
      # Generated dynamically

  bert:
    enabled: true
    category: nlp
    axon_id: "hf/bert-base-uncased@latest"
    description: "BERT base - Masked language model"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  roberta:
    enabled: true
    category: nlp
    axon_id: "hf/roberta-base@latest"
    description: "RoBERTa base - Robust BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  t5:
    enabled: true  # Enabled: Seq2seq support added in Axon v3.1.6 + Core v3.2.9-alpha
    category: nlp
    axon_id: "hf/t5-small@latest"
    description: "T5 small - Text-to-text transformer"
    input_type: text
    notes: "✅ Fully supported: Encoder-decoder architecture enabled in Axon v3.1.6 + Core v3.2.9-alpha"
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Vision Models (Image-based) - Validated with Axon v3.1.6 + Core v3.2.0-alpha
  # =============================================================================
  
  resnet:
    enabled: true
    category: vision
    axon_id: "hf/microsoft/resnet-50@latest"
    description: "ResNet-50 - Image classification (1000 classes) - HuggingFace ONNX"
    input_type: image
    notes: "Using HuggingFace with ONNX conversion. Preprocessing uses torchvision-style (resize 256 + center crop 224)"
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  vit:
    enabled: true
    category: vision
    axon_id: "hf/google/vit-base-patch16-224@latest"
    description: "Vision Transformer (ViT) - Image classification"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  convnext:
    enabled: true
    category: vision
    axon_id: "hf/facebook/convnext-tiny-224@latest"
    description: "ConvNeXt Tiny - Modern CNN architecture"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  mobilenet:
    enabled: true
    category: vision
    axon_id: "hf/google/mobilenet_v2_1.0_224@latest"
    description: "MobileNetV2 - Efficient mobile architecture"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  deit:
    enabled: true
    category: vision
    axon_id: "hf/facebook/deit-small-patch16-224@latest"
    description: "DeiT Small - Data-efficient Image Transformer"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  # Additional vision models (not yet validated)
  swin:
    enabled: false  # Known issue: PyTorch-to-ONNX export fails with dynamic shapes
    category: vision
    axon_id: "hf/microsoft/swin-tiny-patch4-window7-224@latest"
    description: "Swin Transformer - Shifted window attention"
    input_type: image
    notes: "Known PyTorch-to-ONNX conversion issues"
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  efficientnet:
    enabled: true  # Validated and enabled
    category: vision
    axon_id: "hf/google/efficientnet-b0@latest"
    description: "EfficientNet-B0 - Compound scaling CNN"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  # =============================================================================
  # Multimodal Models
  # =============================================================================
  
  clip:
    enabled: true  # Enabled: Multi-encoder support added in Axon v3.1.6 + Core v3.2.9-alpha
    category: multimodal
    axon_id: "hf/openai/clip-vit-base-patch32@latest"
    description: "CLIP - Image-text matching and zero-shot classification"
    input_type: multimodal
    notes: "✅ Fully supported: Multi-encoder architecture (text_model.onnx + vision_model.onnx) enabled in Axon v3.1.6 + Core v3.2.9-alpha"
    small_input:
      text_tokens: 77
      image_width: 224
      image_height: 224
    large_input:
      text_tokens: 77
      image_width: 224
      image_height: 224

  wav2vec2:
    enabled: false  # Blocked: Audio model support in Core
    category: multimodal
    axon_id: "hf/facebook/wav2vec2-base-960h@latest"
    description: "Wav2Vec2 - Speech recognition"
    input_type: audio
    notes: "Audio models require waveform input - pending Core support"
    small_input:
      duration_seconds: 1.0
      sample_rate: 16000
    large_input:
      duration_seconds: 5.0
      sample_rate: 16000

  # =============================================================================
  # LLM Models (GGUF Format) - Native llama.cpp execution
  # =============================================================================
  # These models use Core's GGUF runtime plugin (llama.cpp backend)
  # Requires: Core v3.2.10-alpha+ and Axon v3.1.6+

  tinyllama:
    enabled: true  # Enabled: GGUF runtime plugin ready (Core v3.2.10-alpha)
    category: llm
    format: gguf
    axon_id: "hf/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF@latest"
    model_file: "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"  # Specific quantized variant
    description: "TinyLlama 1.1B - Small but capable chat model"
    input_type: text_generation
    notes: "Native GGUF execution via llama.cpp plugin. 4-bit quantized (~637MB)"
    parameters:
      context_length: 2048
      quantization: "Q4_K_M"
    small_input:
      prompt: "What is the capital of France?"
      max_tokens: 32
    large_input:
      prompt: "Explain the theory of relativity in simple terms."
      max_tokens: 256

  phi2:
    enabled: false  # Disabled: Too large for CI (~1.6GB). Enable for local testing.
    category: llm
    format: gguf
    axon_id: "hf/TheBloke/phi-2-GGUF@latest"
    model_file: "phi-2.Q4_K_M.gguf"
    description: "Microsoft Phi-2 - 2.7B parameter small language model"
    input_type: text_generation
    notes: "Ready for GGUF execution. 4-bit quantized (~1.6GB) - disabled for CI performance"
    parameters:
      context_length: 2048
      quantization: "Q4_K_M"
    small_input:
      prompt: "Write a Python function to calculate fibonacci numbers."
      max_tokens: 64
    large_input:
      prompt: "Explain how neural networks learn through backpropagation."
      max_tokens: 256

  qwen2-0.5b:
    enabled: true  # Enabled: Tested and working. Smallest GGUF model (~380MB)
    category: llm
    format: gguf
    axon_id: "hf/Qwen/Qwen2-0.5B-Instruct-GGUF@latest"
    model_file: "qwen2-0_5b-instruct-q4_k_m.gguf"
    description: "Qwen2 0.5B - Ultra-small instruction-tuned model (Alibaba)"
    input_type: text_generation
    notes: "Tested working. 4-bit quantized (~380MB) - Smallest viable LLM for CI"
    parameters:
      context_length: 32768
      quantization: "Q4_K_M"
    small_input:
      prompt: "What is 2+2?"
      max_tokens: 32
    large_input:
      prompt: "Summarize the benefits of machine learning in healthcare."
      max_tokens: 128

  llama-3.2-1b:
    enabled: true  # Enabled: Meta's latest 1B model (~700MB) - CI-friendly
    category: llm
    format: gguf
    axon_id: "hf/bartowski/Llama-3.2-1B-Instruct-GGUF@latest"
    model_file: "Llama-3.2-1B-Instruct-Q4_K_M.gguf"
    description: "Meta Llama 3.2 1B - Latest small model optimized for mobile"
    input_type: text_generation
    notes: "Meta's latest 1B model (Dec 2024). 4-bit quantized (~700MB)"
    parameters:
      context_length: 131072
      quantization: "Q4_K_M"
    small_input:
      prompt: "What is the capital of France?"
      max_tokens: 32
    large_input:
      prompt: "Explain quantum computing in simple terms."
      max_tokens: 256

  llama-3.2-3b:
    enabled: false  # Disabled: Larger model (~1.8GB). Enable for local testing.
    category: llm
    format: gguf
    axon_id: "hf/bartowski/Llama-3.2-3B-Instruct-GGUF@latest"
    model_file: "Llama-3.2-3B-Instruct-Q4_K_M.gguf"
    description: "Meta Llama 3.2 3B - Excellent quality/size ratio"
    input_type: text_generation
    notes: "Meta's latest 3B model. Best small-model quality. 4-bit quantized (~1.8GB)"
    parameters:
      context_length: 131072
      quantization: "Q4_K_M"
    small_input:
      prompt: "Write a haiku about artificial intelligence."
      max_tokens: 64
    large_input:
      prompt: "Explain the difference between machine learning and deep learning."
      max_tokens: 256

  deepseek-coder-1.3b:
    enabled: true  # Enabled: Code-focused model (~750MB) - DeepSeek coverage
    category: llm
    format: gguf
    axon_id: "hf/TheBloke/deepseek-coder-1.3b-instruct-GGUF@latest"
    model_file: "deepseek-coder-1.3b-instruct.Q4_K_M.gguf"
    description: "DeepSeek Coder 1.3B - Code generation specialist"
    input_type: text_generation
    notes: "Code-focused LLM from DeepSeek. 4-bit quantized (~750MB)"
    parameters:
      context_length: 16384
      quantization: "Q4_K_M"
    small_input:
      prompt: "Write a Python function to reverse a string."
      max_tokens: 64
    large_input:
      prompt: "Implement a binary search tree in Python with insert and search methods."
      max_tokens: 256

  deepseek-llm-7b:
    enabled: false  # Disabled: Full 7B model (~4GB). Enable for high-quality testing.
    category: llm
    format: gguf
    axon_id: "hf/TheBloke/deepseek-llm-7B-chat-GGUF@latest"
    model_file: "deepseek-llm-7b-chat.Q4_K_M.gguf"
    description: "DeepSeek LLM 7B Chat - High-quality open model"
    input_type: text_generation
    notes: "DeepSeek's flagship 7B chat model. 4-bit quantized (~4GB) - Local testing only"
    parameters:
      context_length: 4096
      quantization: "Q4_K_M"
    small_input:
      prompt: "What are the main features of DeepSeek models?"
      max_tokens: 64
    large_input:
      prompt: "Compare and contrast transformer architectures with RNN architectures."
      max_tokens: 256

  # =============================================================================
  # Additional NLP Models (Encoder-only, smaller variants)
  # =============================================================================

  distilbert:
    enabled: true  # Extended testing: Smaller, faster BERT variant
    category: nlp
    axon_id: "hf/distilbert-base-uncased@latest"
    description: "DistilBERT - Smaller, faster BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  albert:
    enabled: true  # Extended testing: Parameter-efficient BERT variant
    category: nlp
    axon_id: "hf/albert-base-v2@latest"
    description: "ALBERT - Parameter-efficient BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Embedding Models
  # =============================================================================

  sentence-transformers:
    enabled: true  # Enabled: Small embedding model for extended testing
    category: nlp
    axon_id: "hf/sentence-transformers/all-MiniLM-L6-v2@latest"
    description: "Sentence-BERT - Text embeddings for semantic search"
    input_type: text
    notes: "Produces 384-dimensional embeddings"
    small_input:
      tokens: 16
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Object Detection / Segmentation Models
  # =============================================================================

  detr:
    enabled: false  # Blocked: Requires bbox output handling in test framework
    category: vision
    axon_id: "hf/facebook/detr-resnet-50@latest"
    description: "DETR - End-to-end object detection with transformer"
    input_type: image
    notes: |
      Blocked: Object detection output format not yet supported.
      Needs: Core bbox parsing, test framework IoU validation, reference annotations.
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 800
      height: 600
      channels: 3

  segformer:
    enabled: false  # Blocked: Requires segmentation mask handling
    category: vision
    axon_id: "hf/nvidia/segformer-b0-finetuned-ade-512-512@latest"
    description: "SegFormer - Semantic segmentation"
    input_type: image
    notes: |
      Blocked: Segmentation mask output format not yet supported.
      Needs: Core 2D mask output, test framework pixel accuracy validation.
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 512
      height: 512
      channels: 3

# =============================================================================
# Multi-Encoder / Encoder-Decoder Support Roadmap
# =============================================================================
# The following models require multi-ONNX-file support in Axon and Core:
#
# 1. CLIP (Multi-Encoder):
#    - Exports: text_model.onnx + vision_model.onnx
#    - Requires: Combined inference orchestration
#
# 2. T5/BART (Encoder-Decoder):
#    - Exports: encoder_model.onnx + decoder_model.onnx + decoder_with_past_model.onnx
#    - Requires: Cross-attention between encoder/decoder
#
# Tracking: https://github.com/mlOS-foundation/axon/issues/TBD
#

# =============================================================================
# Quick reference for adding new models:
# =============================================================================
#
# 1. Add entry under 'models:' section
# 2. Set 'enabled: true' to include in tests
# 3. Choose correct category: nlp, vision, or multimodal
# 4. Provide the Axon model ID (run 'axon search <model>' to find it)
# 5. Configure input sizes appropriate for the model
#
# Vision model notes:
# - Most vision models expect 224x224 inputs (standard ImageNet size)
# - Use smaller sizes (64x64) for quick smoke tests
# - ONNX conversion via Axon v3.1.6+ handles task detection automatically
# - Core v3.2.0-alpha+ supports large image payloads (up to 16MB)
#
# Example:
#   my_new_model:
#     enabled: true
#     category: nlp
#     axon_id: "hf/my-org/my-model@latest"
#     description: "My awesome model"
#     input_type: text
#     small_input:
#       tokens: 10
#     large_input:
#       tokens: 128

