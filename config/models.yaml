# MLOS E2E Test - Model Configuration
# Add new models here to include them in test runs
#
# Updated: 2024-12-07 - Extended format support (Core v3.2.9-alpha PR #39)
# - Seq2seq + Multi-encoder support (Axon v3.1.3, Core v3.2.9-alpha)
# - T5 enabled with encoder-decoder architecture support
# - CLIP enabled with multi-encoder architecture support
# - Format-agnostic runtime plugin system ready
#
# Supported formats (Core PR #39):
#   - ONNX (.onnx) - Built-in via SMI
#   - PyTorch (.pt, .pth, .bin, .safetensors) - Plugin ready
#   - GGUF (.gguf) - llama.cpp LLM plugin ready
#   - TFLite (.tflite) - Plugin ready
#   - TensorFlow (.pb) - Plugin ready
#   - CoreML (.mlmodel, .mlpackage) - Plugin ready

# Test runner settings
settings:
  # Maximum time (seconds) to wait for model installation
  install_timeout: 600
  # Maximum time (seconds) to wait for inference
  inference_timeout: 120  # Increased for vision models (larger inputs)
  # Run large inference tests (more tokens/larger inputs)
  run_large_tests: true
  # Enable LLM generation tests (when GGUF plugin is ready)
  enable_llm_tests: false

# Model definitions
# Each model needs:
#   - enabled: whether to test this model
#   - category: nlp | vision | multimodal
#   - axon_id: the Axon model identifier (hf/namespace/model@version)
#   - input_type: text | image | audio
#   - small_input: input config for small/quick test
#   - large_input: input config for large/stress test

models:
  # =============================================================================
  # NLP Models (Text-based)
  # =============================================================================
  
  gpt2:
    enabled: true
    category: nlp
    axon_id: "hf/distilgpt2@latest"
    description: "DistilGPT-2 - Lightweight text generation"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]  # "What is the capital"
    large_input:
      tokens: 128
      # Generated dynamically

  bert:
    enabled: true
    category: nlp
    axon_id: "hf/bert-base-uncased@latest"
    description: "BERT base - Masked language model"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  roberta:
    enabled: true
    category: nlp
    axon_id: "hf/roberta-base@latest"
    description: "RoBERTa base - Robust BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  t5:
    enabled: true  # Enabled: Seq2seq support added in Axon v3.1.3 + Core v3.2.9-alpha
    category: nlp
    axon_id: "hf/t5-small@latest"
    description: "T5 small - Text-to-text transformer"
    input_type: text
    notes: "✅ Fully supported: Encoder-decoder architecture enabled in Axon v3.1.3 + Core v3.2.9-alpha"
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Vision Models (Image-based) - Validated with Axon v3.1.0 + Core v3.2.0-alpha
  # =============================================================================
  
  resnet:
    enabled: true
    category: vision
    axon_id: "hf/microsoft/resnet-50@latest"
    description: "ResNet-50 - Image classification (1000 classes)"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  vit:
    enabled: true
    category: vision
    axon_id: "hf/google/vit-base-patch16-224@latest"
    description: "Vision Transformer (ViT) - Image classification"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  convnext:
    enabled: true
    category: vision
    axon_id: "hf/facebook/convnext-tiny-224@latest"
    description: "ConvNeXt Tiny - Modern CNN architecture"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  mobilenet:
    enabled: true
    category: vision
    axon_id: "hf/google/mobilenet_v2_1.0_224@latest"
    description: "MobileNetV2 - Efficient mobile architecture"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  deit:
    enabled: true
    category: vision
    axon_id: "hf/facebook/deit-small-patch16-224@latest"
    description: "DeiT Small - Data-efficient Image Transformer"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  # Additional vision models (not yet validated)
  swin:
    enabled: false  # Known issue: PyTorch-to-ONNX export fails with dynamic shapes
    category: vision
    axon_id: "hf/microsoft/swin-tiny-patch4-window7-224@latest"
    description: "Swin Transformer - Shifted window attention"
    input_type: image
    notes: "Known PyTorch-to-ONNX conversion issues"
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  efficientnet:
    enabled: true  # Validated and enabled
    category: vision
    axon_id: "hf/google/efficientnet-b0@latest"
    description: "EfficientNet-B0 - Compound scaling CNN"
    input_type: image
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 224
      height: 224
      channels: 3

  # =============================================================================
  # Multimodal Models
  # =============================================================================
  
  clip:
    enabled: true  # Enabled: Multi-encoder support added in Axon v3.1.3 + Core v3.2.9-alpha
    category: multimodal
    axon_id: "hf/openai/clip-vit-base-patch32@latest"
    description: "CLIP - Image-text matching and zero-shot classification"
    input_type: multimodal
    notes: "✅ Fully supported: Multi-encoder architecture (text_model.onnx + vision_model.onnx) enabled in Axon v3.1.3 + Core v3.2.9-alpha"
    small_input:
      text_tokens: 77
      image_width: 224
      image_height: 224
    large_input:
      text_tokens: 77
      image_width: 224
      image_height: 224

  wav2vec2:
    enabled: false  # Blocked: Audio model support in Core
    category: multimodal
    axon_id: "hf/facebook/wav2vec2-base-960h@latest"
    description: "Wav2Vec2 - Speech recognition"
    input_type: audio
    notes: "Audio models require waveform input - pending Core support"
    small_input:
      duration_seconds: 1.0
      sample_rate: 16000
    large_input:
      duration_seconds: 5.0
      sample_rate: 16000

  # =============================================================================
  # LLM Models (GGUF Format) - Core PR #39 Runtime Plugin System
  # =============================================================================
  # These models require the GGUF runtime plugin (llama.cpp backend)
  # Enable when Core's GGUF plugin is fully integrated

  tinyllama:
    enabled: false  # Pending: GGUF runtime plugin integration
    category: llm
    format: gguf
    axon_id: "hf/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF@latest"
    model_file: "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"  # Specific quantized variant
    description: "TinyLlama 1.1B - Small but capable chat model"
    input_type: text_generation
    notes: "Pending: GGUF runtime plugin. 4-bit quantized (~637MB)"
    parameters:
      context_length: 2048
      quantization: "Q4_K_M"
    small_input:
      prompt: "What is the capital of France?"
      max_tokens: 32
    large_input:
      prompt: "Explain the theory of relativity in simple terms."
      max_tokens: 256

  phi2:
    enabled: false  # Pending: GGUF runtime plugin integration
    category: llm
    format: gguf
    axon_id: "hf/TheBloke/phi-2-GGUF@latest"
    model_file: "phi-2.Q4_K_M.gguf"
    description: "Microsoft Phi-2 - 2.7B parameter small language model"
    input_type: text_generation
    notes: "Pending: GGUF runtime plugin. 4-bit quantized (~1.6GB)"
    parameters:
      context_length: 2048
      quantization: "Q4_K_M"
    small_input:
      prompt: "Write a Python function to calculate fibonacci numbers."
      max_tokens: 64
    large_input:
      prompt: "Explain how neural networks learn through backpropagation."
      max_tokens: 256

  qwen2-0.5b:
    enabled: false  # Pending: GGUF runtime plugin integration
    category: llm
    format: gguf
    axon_id: "hf/Qwen/Qwen2-0.5B-Instruct-GGUF@latest"
    model_file: "qwen2-0_5b-instruct-q4_k_m.gguf"
    description: "Qwen2 0.5B - Ultra-small instruction-tuned model"
    input_type: text_generation
    notes: "Pending: GGUF runtime plugin. 4-bit quantized (~400MB) - Good for CI testing"
    parameters:
      context_length: 32768
      quantization: "Q4_K_M"
    small_input:
      prompt: "Hello, how are you?"
      max_tokens: 32
    large_input:
      prompt: "Summarize the benefits of machine learning in healthcare."
      max_tokens: 128

  # =============================================================================
  # Additional NLP Models (Encoder-only, smaller variants)
  # =============================================================================

  distilbert:
    enabled: true  # Extended testing: Smaller, faster BERT variant
    category: nlp
    axon_id: "hf/distilbert-base-uncased@latest"
    description: "DistilBERT - Smaller, faster BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  albert:
    enabled: true  # Extended testing: Parameter-efficient BERT variant
    category: nlp
    axon_id: "hf/albert-base-v2@latest"
    description: "ALBERT - Parameter-efficient BERT variant"
    input_type: text
    small_input:
      tokens: 7
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Embedding Models
  # =============================================================================

  sentence-transformers:
    enabled: true  # Enabled: Small embedding model for extended testing
    category: nlp
    axon_id: "hf/sentence-transformers/all-MiniLM-L6-v2@latest"
    description: "Sentence-BERT - Text embeddings for semantic search"
    input_type: text
    notes: "Produces 384-dimensional embeddings"
    small_input:
      tokens: 16
      sequence: [101, 2054, 2003, 1996, 3462, 102]
    large_input:
      tokens: 128

  # =============================================================================
  # Object Detection / Segmentation Models
  # =============================================================================

  detr:
    enabled: false  # Blocked: Requires bbox output handling in test framework
    category: vision
    axon_id: "hf/facebook/detr-resnet-50@latest"
    description: "DETR - End-to-end object detection with transformer"
    input_type: image
    notes: |
      Blocked: Object detection output format not yet supported.
      Needs: Core bbox parsing, test framework IoU validation, reference annotations.
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 800
      height: 600
      channels: 3

  segformer:
    enabled: false  # Blocked: Requires segmentation mask handling
    category: vision
    axon_id: "hf/nvidia/segformer-b0-finetuned-ade-512-512@latest"
    description: "SegFormer - Semantic segmentation"
    input_type: image
    notes: |
      Blocked: Segmentation mask output format not yet supported.
      Needs: Core 2D mask output, test framework pixel accuracy validation.
    small_input:
      width: 64
      height: 64
      channels: 3
    large_input:
      width: 512
      height: 512
      channels: 3

# =============================================================================
# Multi-Encoder / Encoder-Decoder Support Roadmap
# =============================================================================
# The following models require multi-ONNX-file support in Axon and Core:
#
# 1. CLIP (Multi-Encoder):
#    - Exports: text_model.onnx + vision_model.onnx
#    - Requires: Combined inference orchestration
#
# 2. T5/BART (Encoder-Decoder):
#    - Exports: encoder_model.onnx + decoder_model.onnx + decoder_with_past_model.onnx
#    - Requires: Cross-attention between encoder/decoder
#
# Tracking: https://github.com/mlOS-foundation/axon/issues/TBD
#

# =============================================================================
# Quick reference for adding new models:
# =============================================================================
#
# 1. Add entry under 'models:' section
# 2. Set 'enabled: true' to include in tests
# 3. Choose correct category: nlp, vision, or multimodal
# 4. Provide the Axon model ID (run 'axon search <model>' to find it)
# 5. Configure input sizes appropriate for the model
#
# Vision model notes:
# - Most vision models expect 224x224 inputs (standard ImageNet size)
# - Use smaller sizes (64x64) for quick smoke tests
# - ONNX conversion via Axon v3.1.0+ handles task detection automatically
# - Core v3.2.0-alpha+ supports large image payloads (up to 16MB)
#
# Example:
#   my_new_model:
#     enabled: true
#     category: nlp
#     axon_id: "hf/my-org/my-model@latest"
#     description: "My awesome model"
#     input_type: text
#     small_input:
#       tokens: 10
#     large_input:
#       tokens: 128

