# Test Input Configuration for E2E Tests
# This file defines test inputs for each model category and specific models
# Inputs are generated using tokenizers/preprocessors for transparency

defaults:
  nlp:
    # Default test sentences for NLP models
    small_text: "Hello world."
    medium_text: "The quick brown fox jumps over the lazy dog. This sentence contains common English words."
    large_text: "Machine learning is a subset of artificial intelligence that enables systems to learn from data. Deep learning models have revolutionized natural language processing and computer vision tasks."
    
  vision:
    # Default image settings for vision models
    image_size: 224
    channels: 3
    normalization: "imagenet"  # mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]

models:
  # ═══════════════════════════════════════════════════════════════════════════
  # NLP Models
  # ═══════════════════════════════════════════════════════════════════════════
  
  gpt2:
    category: nlp
    tokenizer: "distilgpt2"
    test_text:
      small: "Hello, I am a language model."
      medium: "The quick brown fox jumps over the lazy dog. This is a longer sentence for testing."
      large: "Machine learning has transformed how we interact with technology. Language models can now generate human-like text, translate between languages, and answer complex questions."
    max_length:
      small: 16
      medium: 64
      large: 128
    padding: "max_length"
    truncation: true
    required_inputs: ["input_ids"]
    
  bert:
    category: nlp
    tokenizer: "bert-base-uncased"
    test_text:
      small: "Hello world."
      medium: "The quick brown fox jumps over the lazy dog. BERT is great at understanding context."
      large: "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that has achieved state-of-the-art results on many NLP benchmarks."
    max_length:
      small: 16
      medium: 64
      large: 128
    padding: "max_length"
    truncation: true
    # BERT requires: input_ids, attention_mask, token_type_ids
    required_inputs: ["input_ids", "attention_mask", "token_type_ids"]
    
  roberta:
    category: nlp
    tokenizer: "roberta-base"
    test_text:
      small: "Hello world."
      medium: "RoBERTa is a robustly optimized BERT pretraining approach."
      large: "RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, removing the next-sentence prediction objective and training with much larger mini-batches."
    max_length:
      small: 16
      medium: 64
      large: 128
    padding: "max_length"
    truncation: true
    # RoBERTa ONNX model only needs input_ids
    required_inputs: ["input_ids"]
    
  t5:
    category: nlp
    tokenizer: "t5-small"
    test_text:
      small: "translate English to German: Hello"
      medium: "summarize: The quick brown fox jumps over the lazy dog multiple times."
      large: "summarize: Machine learning is transforming industries worldwide."
    max_length:
      small: 16
      medium: 64
      large: 128
    padding: "max_length"
    truncation: true
    required_inputs: ["input_ids", "attention_mask", "decoder_input_ids"]
    decoder_start_token_id: 0

  distilbert:
    category: nlp
    tokenizer: "distilbert-base-uncased"
    test_text:
      small: "Hello world."
      medium: "The quick brown fox jumps over the lazy dog."
      large: "DistilBERT is a smaller, faster, cheaper version of BERT that retains 97% of BERT's language understanding."
    max_length:
      small: 16
      medium: 64
      large: 128
    padding: "max_length"
    truncation: true
    # DistilBERT requires: input_ids, attention_mask
    required_inputs: ["input_ids", "attention_mask"]

  albert:
    category: nlp
    tokenizer: "albert-base-v2"
    test_text:
      small: "Hello world."
      medium: "The quick brown fox jumps over the lazy dog."
      large: "ALBERT is a lite BERT with parameter-reduction techniques for efficient language understanding."
    max_length:
      small: 16
      medium: 64
      large: 128
    padding: "max_length"
    truncation: true
    # ALBERT requires: input_ids, attention_mask, token_type_ids
    required_inputs: ["input_ids", "attention_mask", "token_type_ids"]

  sentence-transformers:
    category: nlp
    tokenizer: "sentence-transformers/all-MiniLM-L6-v2"
    test_text:
      small: "This is a sentence for embedding."
      medium: "Sentence transformers produce dense vector representations for sentences."
      large: "SBERT is designed to produce semantically meaningful sentence embeddings that can be compared using cosine similarity."
    max_length:
      small: 128
      medium: 128
      large: 128
    padding: "max_length"
    truncation: true
    # Sentence-transformers requires: input_ids, attention_mask, token_type_ids
    required_inputs: ["input_ids", "attention_mask", "token_type_ids"]

  # ═══════════════════════════════════════════════════════════════════════════
  # Vision Models
  # ═══════════════════════════════════════════════════════════════════════════
  
  resnet:
    category: vision
    preprocessor: "microsoft/resnet-50"
    input_name: "pixel_values"
    image_size: 224
    channels: 3
    # ImageNet normalization
    normalization:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    # Generate random normalized image data
    test_seed: 42  # For reproducibility
    
  vit:
    category: vision
    preprocessor: "google/vit-base-patch16-224"
    input_name: "pixel_values"
    image_size: 224
    channels: 3
    normalization:
      mean: [0.5, 0.5, 0.5]
      std: [0.5, 0.5, 0.5]
    test_seed: 42
    
  convnext:
    category: vision
    preprocessor: "facebook/convnext-tiny-224"
    input_name: "pixel_values"
    image_size: 224
    channels: 3
    normalization:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    test_seed: 42
    
  mobilenet:
    category: vision
    preprocessor: "google/mobilenet_v2_1.0_224"
    input_name: "pixel_values"
    image_size: 224
    channels: 3
    normalization:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    test_seed: 42
    
  deit:
    category: vision
    preprocessor: "facebook/deit-small-patch16-224"
    input_name: "pixel_values"
    image_size: 224
    channels: 3
    normalization:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    test_seed: 42

  efficientnet:
    category: vision
    preprocessor: "google/efficientnet-b0"
    input_name: "pixel_values"
    image_size: 224
    channels: 3
    normalization:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    test_seed: 42

  # ═══════════════════════════════════════════════════════════════════════════
  # Multi-Modal Models
  # ═══════════════════════════════════════════════════════════════════════════
  
  clip:
    category: multimodal
    model_type: clip
    text_tokenizer: "openai/clip-vit-base-patch32"
    test_text:
      small: "a photo of a cat"
      medium: "a beautiful photograph of a golden retriever dog sitting in a garden"
      large: "a high resolution professional photograph of a majestic lion resting on a rocky outcrop in the african savanna at sunset"
    text_max_length:
      small: 77
      medium: 77
      large: 77
    image_size:
      small: 224
      medium: 224
      large: 224
    channels: 3
    test_seed: 42
    # CLIP normalization
    normalization:
      mean: [0.48145466, 0.4578275, 0.40821073]
      std: [0.26862954, 0.26130258, 0.27577711]
    required_inputs: ["input_ids", "attention_mask", "pixel_values"]
  
  wav2vec2:
    category: multimodal
    model_type: audio
    input_name: "input_values"
    sample_rate: 16000
    duration:
      small: 1.0    # 1 second
      medium: 3.0   # 3 seconds
      large: 5.0    # 5 seconds
    test_seed: 42
    required_inputs: ["input_values"]

  # ═══════════════════════════════════════════════════════════════════════════
  # LLM Models (GGUF Format)
  # ═══════════════════════════════════════════════════════════════════════════
  # These models use the GGUF format and are executed via Core's llama.cpp plugin.
  # Input format: {"prompt": "...", "max_tokens": N, "temperature": 0.7}

  tinyllama:
    category: llm
    format: gguf
    prompts:
      # Must match golden-test-data.yaml for validation to pass
      small: "What is the capital of France?"
      medium: "What is 2 + 2?"
      large: "Write a detailed explanation of the theory of relativity and its implications for modern physics."
    max_tokens:
      small: 32
      medium: 16
      large: 256
    temperature: 0.1
    top_p: 0.9

  phi2:
    category: llm
    format: gguf
    prompts:
      # Must match golden-test-data.yaml for validation to pass
      small: "Write a Python function to calculate fibonacci numbers."
      medium: "Explain the concept of recursion in programming with examples."
      large: "Explain how neural networks learn through backpropagation, including the mathematical foundations."
    max_tokens:
      small: 64
      medium: 256
      large: 512
    temperature: 0.1
    top_p: 0.9

  qwen2-0.5b:
    category: llm
    format: gguf
    prompts:
      # Must match golden-test-data.yaml for validation to pass
      small: "What is 2 + 2? Answer with just the number."
      medium: "Explain the concept of recursion in programming."
      large: "Summarize the key developments in artificial intelligence over the past decade."
    max_tokens:
      small: 8
      medium: 128
      large: 256
    temperature: 0.1
    top_p: 0.9

  llama-3.2-1b:
    category: llm
    format: gguf
    prompts:
      # Must match golden-test-data.yaml for validation to pass
      small: "What is the capital of Japan? Answer in one word."
      medium: "What is 7 + 8? Answer with just the number."
      large: "Explain the principles of machine learning in simple terms."
    max_tokens:
      small: 16
      medium: 8
      large: 256
    temperature: 0.1
    top_p: 0.9

  deepseek-coder-1.3b:
    category: llm
    format: gguf
    prompts:
      # Must match golden-test-data.yaml for validation to pass
      small: "Write a Python function called 'add' that takes two numbers and returns their sum."
      medium: "Write a Python one-liner to reverse a string s."
      large: "Write a Python function to implement binary search on a sorted list."
    max_tokens:
      small: 64
      medium: 32
      large: 256
    temperature: 0.1
    top_p: 0.9

